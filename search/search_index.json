{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-flock/","text":"Submit Node Flocking to OSG \u00b6 This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Submit Node Flocking to OSG "},{"location":"hpc_administration/osg_for_hpc_administrators/osg-flock/#submit-node-flocking-to-osg","text":"This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Submit Node Flocking to OSG"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/","text":"Running OSG jobs on XSEDE \u00b6 Overview \u00b6 The OSG promotes science by: enabling a framework of distributed computing and storage resources making available a set of services and methods that enable better access to ever increasing computing resources for researchers and communities providing resource sharing principles and software that enable distributed High Throughput Computing (dHTC) for users and communities at all scales. The OSG facilitates access to dHTC for scientific research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG Consortium ; an overview is available at An Introduction to OSG . In 2017, OSG is comprised of about 126 institutions with ~120 active sites that collectively support usage of ~4,000,000 core hours per day. Up-to-date usage metrics are available at the OSG Usage Display . Cores that are not being used at any point in time by the owning communities are made available for shared use by other researchers; this usage mode is called opportunistic access. OSG supports XSEDE users by providing a Virtual Cluster that forms an abstraction layer to access the opportunistic cores in the distributed OSG infrastructure. This interface allows XSEDE users to view the OSG as a single cluster to manage their jobs, provide the inputs and retrieve the outputs. XSEDE users access the OSG via the OSG-XSEDE login host that appears as a resource in the XSEDE infrastructure. Computation that is a good match for OSG \u00b6 High throughput workflows with simple system and data dependencies are a good fit for OSG. The Condor manual has an overview of high throughput computing . Jobs submitted into the OSG Virtual Cluster will be executed on machines at several remote physical clusters. These machines may differ in terms of computing environment from the submit node. Therefore it is important that the jobs are as self-contained as possible by generic binaries and data that can be either carried with the job, or staged on demand. Please consider the following guidelines: Software should preferably be single threaded , using less than 2 GB memory and each invocation should run for 1-12 hours . Please contact the support listed below for more information about these capabilities. System level check pointing, such as the HTCondor standard universe, is not available. Application level check pointing, for example applications writing out state and restart files, can be made to work on the system. Compute sites in the OSG can be configured to use pre-emption, which means jobs can be automatically killed if higher priority jobs enter the system. Pre-empted jobs will restart on another site, but it is important that the jobs can handle multiple restarts. Binaries should preferably be statically linked. However, dynamically linked binaries with standard library dependencies, built for a 64-bit Red Hat Enterprise Linux (RHEL) 6 machines will also work. Also, interpreted languages such as Python or Perl will work as long as there are no special module requirements. Input and output data for each job should be < 10 GB to allow them to be pulled in by the jobs, processed and pushed back to the submit node. Note that the OSG Virtual Cluster does not currently have a global shared file system, so jobs with such dependencies will not work. Software dependencies can be difficult to accommodate unless the software can be staged with the job. The following are examples of computations that are not good matches for OSG: Tightly coupled computations, for example MPI based communication, will not work well on OSG due to the distributed nature of the infrastructure. Computations requiring a shared file system will not work, as there is no shared filesystem between the different clusters on OSG. Computations requiring complex software deployments are not a good fit. There is limited support for distributing software to the compute clusters, but for complex software, or licensed software, deployment can be a major task. System Configuration \u00b6 The OSG Virtual Cluster is a Condor pool overlay on top of OSG resources. The pool is dynamically sized based on the demand, the number of jobs in the queue, and supply, resource availability at the OSG resources. It is expected that the average number of resources, on average, available to XSEDE users will be in the order of 1,000 cores. One important difference between the OSG Virtual Cluster and most of the other XSEDE resources is that the OSG Virtual Cluster does not have a shared file system. This means that your jobs will have to bring executables and input data. Condor can transfer the files for you, but you will have to identify and list the files in your Condor job description file. Local storage space at the submission site is controlled by quota. Your home directory has a quota of 10 GBs and your work directory /local-scratch/$USER has a quota of 1 TB. There are no global quotas on the remote compute nodes, but expect that about 10 GBs are available as scratch space for each job. System Access \u00b6 The preferred method to access the system is via the XSEDE Single Sign On (SSO) Hub. Please see the sso documentation for details. A secondary access methor is to use SSH public key authentication. Secure shell users should feel free to append their public RSA key to their ~/.ssh/authorized_keys file to enable access from their own computer. Please login once via the SSO Hub to install your key. Please make sure that the permissions on the .ssh directory and the authorized_keys file have appropiate permissions. For example $ chmod 755 ~/.ssh $ chmod 644 ~/.ssh/authorized_keys Application Development< \u00b6 Most of the clusters in OSG are running Red Hat Enterprise Linux (RHEL) 6 or 7, or some derivative thereof, on an x86_64 architecture. For your application to work well in this environment, it is recommend that the application is compiled on a similar system, for example on the OSG Virtual Cluster login system: submit-1.osg.xsede.org . It is also recommended that the application be statically linked, or alternatively dynamically linked against just a few standard libraries. What libraries a binary depends on can be checked using the Unix ldd command-line utility: $ ldd a.out a.out is a static executable In the case of interpreted languages like Python and Perl, applications have to either use only standard modules, or be able to ship the modules with the jobs. Please note that different compute nodes might have different versions of these tools installed. A good solution to complex software stack is Singularity containers which are described below. Running Your Application \u00b6 The OSG Virtual Cluster is based on Condor and the Condor manual provides a reference for command line tools. The commonly used tools are: **condor_submit** - Takes a Condor submit file and adds the job to the queue **condor_q** - Lists the jobs in the queue. Can be invoked with your username to limit the list of jobs to your jobs: condor_q $USER **condor_status** - Lists the available slots in the system. Note that this is a dynamic list and if there are no jobs in the system, condor_status may return an empty list **condor_rm** - Remove a job from the queue. If you are running a DAG, please condor_rm the id of the DAG to remove the whole workflow. Submitting a Simple Job \u00b6 Below is a basic job description for the Virtual Cluster. universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 1 request_memory = 2 GB request_disk = 10 GB executable = /bin/hostname arguments = -f should_transfer_files = YES WhenToTransferOutput = ON_EXIT output = job.out error = job.err log = job.log notification = NEVER queue Create a file named job.condor containing the above text and then run: $ condor_submit job.condor You can check the status of the job using the condor_q command. Note: The Open Science Pool is a distributed resource, and there will be minor differences in the compute nodes, for example in what system libraries and tools are installed. Therefore, when running a large number of jobs, it is important to detect and handle job failures correctly in an automatic fashion. It is recommended that your application uses non-zero exit code convention to indicate failures, and that you enable retries in your Condor submit files. For example: # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts &lt; 3) &amp;&amp; ((CurrentTime - EnteredCurrentStatus) &gt; (60*60)) Job Example: Java with a job wrapper \u00b6 The following is an example on how to run Java code on Open Science Pool. The job requirements specifies that the job requires Java, and a wrapper script is used to invoke Java. File: condor.sub universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = HAS_JAVA == True # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts < 3) && ((CurrentTime - EnteredCurrentStatus) > (60*60)) executable = wrapper.sh should_transfer_files = YES WhenToTransferOutput = ON_EXIT # a list of files that the job needs transfer_input_files = HelloWorld.jar output = job.out error = job.err log = job.log notification = NEVER queue File: wrapper.sh 1 2 3 4 5 #!/bin/bash set -e java HelloWorld Sample Jobs and Workflows \u00b6 A set of sample jobs and workflows can be found under /opt/sample-jobs on the submit-1.osg.xsede.org host. README files are included with details for each sample. /opt/sample-jobs/single/ contains a single Condor job example. Single jobs can be used for smaller set of jobs or if the job structure is simple, such as parameter sweeps. A sample-app package ( sampleapp.tgz ) is available in the /opt/sample-jobs/sampleapp/ directory. This package shows how to build a library and an executable, both with dynamic and static linking, and submit the job to a set of different schedulers available on XSEDE. The package includes submit files for PBS, SGE and Condor. DAGMan is a HTCondor workflow tool. It allows the creation of a directed acyclic graph of jobs to be run, and then DAGMan submits and manages the jobs. DAGMan is also useful if you have a large number of jobs, even if there are no job inter-dependencies, as DAGMan can keep track of failures and provide a restart mechanism if part of the workflow fails. A sample DAGMan workflow can be found in /opt/sample-jobs/dag/ Pegasus is a workflow system that can be used for more complex workflows. It plans abstract workflow representations down to an executable workflow and uses Condor DAGMan as a workflow executor. Pegasus also provides debugging and monitoring tools that allow users to easily track failures in their workflows. Workflow provenance information is collected and can be summarized with the provided statistical and plotting tools. A sample Pegasus workflow can be found in /opt/sample-jobs/pegasus/ . Singularity Containers \u00b6 Singularity containers provide a great solution for complex software stacks or OS requirements, and OSG has easy to use integrated support for such containers. Full details can be found in the Singularity Containers . Distribute data with Stash \u00b6 Stash is a data solution used under OSGConnect , but is partly also available for OSG XSEDE users. Files under /local-scratch/public_stash/ will automatically synchronize to the globally available /cvmfs/stash.osgstorage.org/user/xd-login/public/ file system, which is available to the majority of OSG connected compute nodes. This is a great way to distribute software and commonly used data sets. To get started, create your own sub directory: $ mkdir -p /local-scratch/public_stash/$USER Now, populate that directory with the software and data required for your jobs. The synchronization can take couple of hours. You can verify the data has reached the /cvmfs system by using ls : $ ls /cvmfs/stash.osgstorage.org/user/xd-login/public/ To steer your jobs to compute nodes which can access the file system, add HAS_CVMFS_stash_osgstorage_org == True to your job requirements. For example: requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True && HAS_CVMFS_stash_osgstorage_org == True Once a job starts up on such a compute node, the job can read directly from /cvmfs/stash.osgstorage.org/user/xd-login/public/ How to get help using OSG \u00b6 XSEDE users of OSG may get technical support by contacting OSG Research Facilitation staff at email support@opensciencegrid.org . Users may also contact the XSEDE helpdesk .","title":"Running OSG jobs on XSEDE"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#running-osg-jobs-on-xsede","text":"","title":"Running OSG jobs on XSEDE"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#overview","text":"The OSG promotes science by: enabling a framework of distributed computing and storage resources making available a set of services and methods that enable better access to ever increasing computing resources for researchers and communities providing resource sharing principles and software that enable distributed High Throughput Computing (dHTC) for users and communities at all scales. The OSG facilitates access to dHTC for scientific research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG Consortium ; an overview is available at An Introduction to OSG . In 2017, OSG is comprised of about 126 institutions with ~120 active sites that collectively support usage of ~4,000,000 core hours per day. Up-to-date usage metrics are available at the OSG Usage Display . Cores that are not being used at any point in time by the owning communities are made available for shared use by other researchers; this usage mode is called opportunistic access. OSG supports XSEDE users by providing a Virtual Cluster that forms an abstraction layer to access the opportunistic cores in the distributed OSG infrastructure. This interface allows XSEDE users to view the OSG as a single cluster to manage their jobs, provide the inputs and retrieve the outputs. XSEDE users access the OSG via the OSG-XSEDE login host that appears as a resource in the XSEDE infrastructure.","title":"Overview"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#computation-that-is-a-good-match-for-osg","text":"High throughput workflows with simple system and data dependencies are a good fit for OSG. The Condor manual has an overview of high throughput computing . Jobs submitted into the OSG Virtual Cluster will be executed on machines at several remote physical clusters. These machines may differ in terms of computing environment from the submit node. Therefore it is important that the jobs are as self-contained as possible by generic binaries and data that can be either carried with the job, or staged on demand. Please consider the following guidelines: Software should preferably be single threaded , using less than 2 GB memory and each invocation should run for 1-12 hours . Please contact the support listed below for more information about these capabilities. System level check pointing, such as the HTCondor standard universe, is not available. Application level check pointing, for example applications writing out state and restart files, can be made to work on the system. Compute sites in the OSG can be configured to use pre-emption, which means jobs can be automatically killed if higher priority jobs enter the system. Pre-empted jobs will restart on another site, but it is important that the jobs can handle multiple restarts. Binaries should preferably be statically linked. However, dynamically linked binaries with standard library dependencies, built for a 64-bit Red Hat Enterprise Linux (RHEL) 6 machines will also work. Also, interpreted languages such as Python or Perl will work as long as there are no special module requirements. Input and output data for each job should be < 10 GB to allow them to be pulled in by the jobs, processed and pushed back to the submit node. Note that the OSG Virtual Cluster does not currently have a global shared file system, so jobs with such dependencies will not work. Software dependencies can be difficult to accommodate unless the software can be staged with the job. The following are examples of computations that are not good matches for OSG: Tightly coupled computations, for example MPI based communication, will not work well on OSG due to the distributed nature of the infrastructure. Computations requiring a shared file system will not work, as there is no shared filesystem between the different clusters on OSG. Computations requiring complex software deployments are not a good fit. There is limited support for distributing software to the compute clusters, but for complex software, or licensed software, deployment can be a major task.","title":"Computation that is a good match for OSG"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#system-configuration","text":"The OSG Virtual Cluster is a Condor pool overlay on top of OSG resources. The pool is dynamically sized based on the demand, the number of jobs in the queue, and supply, resource availability at the OSG resources. It is expected that the average number of resources, on average, available to XSEDE users will be in the order of 1,000 cores. One important difference between the OSG Virtual Cluster and most of the other XSEDE resources is that the OSG Virtual Cluster does not have a shared file system. This means that your jobs will have to bring executables and input data. Condor can transfer the files for you, but you will have to identify and list the files in your Condor job description file. Local storage space at the submission site is controlled by quota. Your home directory has a quota of 10 GBs and your work directory /local-scratch/$USER has a quota of 1 TB. There are no global quotas on the remote compute nodes, but expect that about 10 GBs are available as scratch space for each job.","title":"System Configuration"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#system-access","text":"The preferred method to access the system is via the XSEDE Single Sign On (SSO) Hub. Please see the sso documentation for details. A secondary access methor is to use SSH public key authentication. Secure shell users should feel free to append their public RSA key to their ~/.ssh/authorized_keys file to enable access from their own computer. Please login once via the SSO Hub to install your key. Please make sure that the permissions on the .ssh directory and the authorized_keys file have appropiate permissions. For example $ chmod 755 ~/.ssh $ chmod 644 ~/.ssh/authorized_keys","title":"System Access"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#application-development","text":"Most of the clusters in OSG are running Red Hat Enterprise Linux (RHEL) 6 or 7, or some derivative thereof, on an x86_64 architecture. For your application to work well in this environment, it is recommend that the application is compiled on a similar system, for example on the OSG Virtual Cluster login system: submit-1.osg.xsede.org . It is also recommended that the application be statically linked, or alternatively dynamically linked against just a few standard libraries. What libraries a binary depends on can be checked using the Unix ldd command-line utility: $ ldd a.out a.out is a static executable In the case of interpreted languages like Python and Perl, applications have to either use only standard modules, or be able to ship the modules with the jobs. Please note that different compute nodes might have different versions of these tools installed. A good solution to complex software stack is Singularity containers which are described below.","title":"Application Development&lt;"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#running-your-application","text":"The OSG Virtual Cluster is based on Condor and the Condor manual provides a reference for command line tools. The commonly used tools are: **condor_submit** - Takes a Condor submit file and adds the job to the queue **condor_q** - Lists the jobs in the queue. Can be invoked with your username to limit the list of jobs to your jobs: condor_q $USER **condor_status** - Lists the available slots in the system. Note that this is a dynamic list and if there are no jobs in the system, condor_status may return an empty list **condor_rm** - Remove a job from the queue. If you are running a DAG, please condor_rm the id of the DAG to remove the whole workflow.","title":"Running Your Application"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#submitting-a-simple-job","text":"Below is a basic job description for the Virtual Cluster. universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 1 request_memory = 2 GB request_disk = 10 GB executable = /bin/hostname arguments = -f should_transfer_files = YES WhenToTransferOutput = ON_EXIT output = job.out error = job.err log = job.log notification = NEVER queue Create a file named job.condor containing the above text and then run: $ condor_submit job.condor You can check the status of the job using the condor_q command. Note: The Open Science Pool is a distributed resource, and there will be minor differences in the compute nodes, for example in what system libraries and tools are installed. Therefore, when running a large number of jobs, it is important to detect and handle job failures correctly in an automatic fashion. It is recommended that your application uses non-zero exit code convention to indicate failures, and that you enable retries in your Condor submit files. For example: # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts &lt; 3) &amp;&amp; ((CurrentTime - EnteredCurrentStatus) &gt; (60*60))","title":"Submitting a Simple Job"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#job-example-java-with-a-job-wrapper","text":"The following is an example on how to run Java code on Open Science Pool. The job requirements specifies that the job requires Java, and a wrapper script is used to invoke Java. File: condor.sub universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = HAS_JAVA == True # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts < 3) && ((CurrentTime - EnteredCurrentStatus) > (60*60)) executable = wrapper.sh should_transfer_files = YES WhenToTransferOutput = ON_EXIT # a list of files that the job needs transfer_input_files = HelloWorld.jar output = job.out error = job.err log = job.log notification = NEVER queue File: wrapper.sh 1 2 3 4 5 #!/bin/bash set -e java HelloWorld","title":"Job Example: Java with a job wrapper"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#sample-jobs-and-workflows","text":"A set of sample jobs and workflows can be found under /opt/sample-jobs on the submit-1.osg.xsede.org host. README files are included with details for each sample. /opt/sample-jobs/single/ contains a single Condor job example. Single jobs can be used for smaller set of jobs or if the job structure is simple, such as parameter sweeps. A sample-app package ( sampleapp.tgz ) is available in the /opt/sample-jobs/sampleapp/ directory. This package shows how to build a library and an executable, both with dynamic and static linking, and submit the job to a set of different schedulers available on XSEDE. The package includes submit files for PBS, SGE and Condor. DAGMan is a HTCondor workflow tool. It allows the creation of a directed acyclic graph of jobs to be run, and then DAGMan submits and manages the jobs. DAGMan is also useful if you have a large number of jobs, even if there are no job inter-dependencies, as DAGMan can keep track of failures and provide a restart mechanism if part of the workflow fails. A sample DAGMan workflow can be found in /opt/sample-jobs/dag/ Pegasus is a workflow system that can be used for more complex workflows. It plans abstract workflow representations down to an executable workflow and uses Condor DAGMan as a workflow executor. Pegasus also provides debugging and monitoring tools that allow users to easily track failures in their workflows. Workflow provenance information is collected and can be summarized with the provided statistical and plotting tools. A sample Pegasus workflow can be found in /opt/sample-jobs/pegasus/ .","title":"Sample Jobs and Workflows"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#singularity-containers","text":"Singularity containers provide a great solution for complex software stacks or OS requirements, and OSG has easy to use integrated support for such containers. Full details can be found in the Singularity Containers .","title":"Singularity Containers"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#distribute-data-with-stash","text":"Stash is a data solution used under OSGConnect , but is partly also available for OSG XSEDE users. Files under /local-scratch/public_stash/ will automatically synchronize to the globally available /cvmfs/stash.osgstorage.org/user/xd-login/public/ file system, which is available to the majority of OSG connected compute nodes. This is a great way to distribute software and commonly used data sets. To get started, create your own sub directory: $ mkdir -p /local-scratch/public_stash/$USER Now, populate that directory with the software and data required for your jobs. The synchronization can take couple of hours. You can verify the data has reached the /cvmfs system by using ls : $ ls /cvmfs/stash.osgstorage.org/user/xd-login/public/ To steer your jobs to compute nodes which can access the file system, add HAS_CVMFS_stash_osgstorage_org == True to your job requirements. For example: requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True && HAS_CVMFS_stash_osgstorage_org == True Once a job starts up on such a compute node, the job can read directly from /cvmfs/stash.osgstorage.org/user/xd-login/public/","title":"Distribute data with Stash"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-xsede/#how-to-get-help-using-osg","text":"XSEDE users of OSG may get technical support by contacting OSG Research Facilitation staff at email support@opensciencegrid.org . Users may also contact the XSEDE helpdesk .","title":"How to get help using OSG"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/","text":"Submit Workflows with HTCondor's DAGMan \u00b6 Overview \u00b6 If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. A simple workflow manager that integrates with HTCondor is DAGMan, or \"DAG Manager\" where DAG stands for the typical picture of a workflow, a directed acyclic graph. Learning Resources \u00b6 This talk (originally presented at HTCondor Week 2020) gives a good overview of when to use DAGMan and its most useful features: For full details on various DAGMan features, see the HTCondor manual page: DAGMan Manual Page","title":"Submit Workflows with HTCondor's DAGMan "},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#submit-workflows-with-htcondors-dagman","text":"","title":"Submit Workflows with HTCondor's DAGMan"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#overview","text":"If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. A simple workflow manager that integrates with HTCondor is DAGMan, or \"DAG Manager\" where DAG stands for the typical picture of a workflow, a directed acyclic graph.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#learning-resources","text":"This talk (originally presented at HTCondor Week 2020) gives a good overview of when to use DAGMan and its most useful features: For full details on various DAGMan features, see the HTCondor manual page: DAGMan Manual Page","title":"Learning Resources"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/","text":"Using GPUs on the OSPool \u00b6 The Open Science Pool has an increasing number of GPUs available to run jobs. Requesting GPUs \u00b6 To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Currently, a job can only use 1 GPU at the time. You should only request a GPU if your software has been written to use a GPU. It is also worth running test jobs on a GPU versus CPUs-only, to observe the amount of speed up. Specific GPU Requests \u00b6 HTCondor records different GPU attributes that can be used to select specific types of GPU devices. A few attributes that may be useful: CUDACapability : this is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability\" CUDADriverVersion : maximum version of the CUDA libraries that can be supported on the GPU CUDAGlobalMemoryMb : amount of GPU memory available on the GPU device Any of the attributes above can be used in the submit file's requirements line to select a specific kind of GPU. For example, to request a GPU with more than 8GB of GPU memory, one could use: requirements = (CUDAGlobalMemoryMb >= 8192) If you want a certain type or family of GPUs, we usually recommend using the GPU's 'Compute Capability', known as the CUDACapability by HTCondor. An A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: requirements = (CUDACapability == 8.0) Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity. Available GPUs \u00b6 Just like CPUs, GPUs are shared with the OSG community only when the resource is idle. Therefore, we do not know exactly what resources are available at what time. When requesting a GPU job, you might land on one of the following types of GPUs: Tesla K20m, K40m (CUDACapability: 3.5) Quadro M5000 (CUDACapability: 5.2) GeForce GTX 1080 Ti (CUDACapability: 6.1) V100 (CUDACapability: 7.0) Quadro RTX 6000 (CUDACapability: 7.5) A100 (CUDACapability: 8.0) A40 (CUDACapability: 8.6) Software and Data Considerations \u00b6 Software for GPUs \u00b6 For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Using Containers on the OSPool Sample TensorFlow GPU Container Image Definition TensorFlow Example Job Data Needs for GPU Jobs \u00b6 As with any kind of job submission, check your data sizes (per job) before submitting jobs and choose the appropriate file transfer method for your data. See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"Using GPUs on the OSPool "},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#using-gpus-on-the-ospool","text":"The Open Science Pool has an increasing number of GPUs available to run jobs.","title":"Using GPUs on the OSPool"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#requesting-gpus","text":"To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Currently, a job can only use 1 GPU at the time. You should only request a GPU if your software has been written to use a GPU. It is also worth running test jobs on a GPU versus CPUs-only, to observe the amount of speed up.","title":"Requesting GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#specific-gpu-requests","text":"HTCondor records different GPU attributes that can be used to select specific types of GPU devices. A few attributes that may be useful: CUDACapability : this is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability\" CUDADriverVersion : maximum version of the CUDA libraries that can be supported on the GPU CUDAGlobalMemoryMb : amount of GPU memory available on the GPU device Any of the attributes above can be used in the submit file's requirements line to select a specific kind of GPU. For example, to request a GPU with more than 8GB of GPU memory, one could use: requirements = (CUDAGlobalMemoryMb >= 8192) If you want a certain type or family of GPUs, we usually recommend using the GPU's 'Compute Capability', known as the CUDACapability by HTCondor. An A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: requirements = (CUDACapability == 8.0) Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity.","title":"Specific GPU Requests"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#available-gpus","text":"Just like CPUs, GPUs are shared with the OSG community only when the resource is idle. Therefore, we do not know exactly what resources are available at what time. When requesting a GPU job, you might land on one of the following types of GPUs: Tesla K20m, K40m (CUDACapability: 3.5) Quadro M5000 (CUDACapability: 5.2) GeForce GTX 1080 Ti (CUDACapability: 6.1) V100 (CUDACapability: 7.0) Quadro RTX 6000 (CUDACapability: 7.5) A100 (CUDACapability: 8.0) A40 (CUDACapability: 8.6)","title":"Available GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#software-and-data-considerations","text":"","title":"Software and Data Considerations"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#software-for-gpus","text":"For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Using Containers on the OSPool Sample TensorFlow GPU Container Image Definition TensorFlow Example Job","title":"Software for GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#data-needs-for-gpu-jobs","text":"As with any kind of job submission, check your data sizes (per job) before submitting jobs and choose the appropriate file transfer method for your data. See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"Data Needs for GPU Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/large-memory-jobs/","text":"Large Memory Jobs \u00b6 By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs "},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/large-memory-jobs/#large-memory-jobs","text":"By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/multicore-jobs/","text":"Multicore Jobs \u00b6 Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job.","title":"Multicore Jobs "},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/multicore-jobs/#multicore-jobs","text":"Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job.","title":"Multicore Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/openmpi-jobs/","text":"OpenMPI Jobs \u00b6 Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources). To get started, first compile your code using the OpenMPI in the modules system. For example: $ module load openmpi $ mpicc -o hello hello.c You can test the executable locally using mpiexec : $ mpiexec -n 4 hello Hello world from process 1 of 4 Hello world from process 3 of 4 Hello world from process 0 of 4 Hello world from process 2 of 4 To run your code as a job on the Open Science Pool, first create a wrapper.sh . Example: 1 2 3 4 5 6 7 #!/bin/bash set -e module load openmpi mpiexec -n 4 hello Then, a job submit file: Requirements = OSGVO_OS_STRING == \"RHEL 7\" && TARGET.Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 4 request_memory = 4 GB executable = wrapper.sh transfer_input_files = hello output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"OpenMPI Jobs "},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/openmpi-jobs/#openmpi-jobs","text":"Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources). To get started, first compile your code using the OpenMPI in the modules system. For example: $ module load openmpi $ mpicc -o hello hello.c You can test the executable locally using mpiexec : $ mpiexec -n 4 hello Hello world from process 1 of 4 Hello world from process 3 of 4 Hello world from process 0 of 4 Hello world from process 2 of 4 To run your code as a job on the Open Science Pool, first create a wrapper.sh . Example: 1 2 3 4 5 6 7 #!/bin/bash set -e module load openmpi mpiexec -n 4 hello Then, a job submit file: Requirements = OSGVO_OS_STRING == \"RHEL 7\" && TARGET.Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 4 request_memory = 4 GB executable = wrapper.sh transfer_input_files = hello output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"OpenMPI Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/","text":"Indicate the Duration Category of Your Jobs \u00b6 Why Job Duration Categories? \u00b6 To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted via an OSG Connect access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Specify a Job Duration Category \u00b6 The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below. Test Jobs for Expected Duration \u00b6 As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@opensciencegrid.org Maximum Allowed Duration \u00b6 Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@opensciencegrid.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter. Self-Checkpointing Jobs \u00b6 Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@opensciencegrid.org if you have questions. :)","title":"Indicate the Duration Category of Your Jobs "},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#indicate-the-duration-category-of-your-jobs","text":"","title":"Indicate the Duration Category of Your Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#why-job-duration-categories","text":"To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted via an OSG Connect access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput.","title":"Why Job Duration Categories?"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#specify-a-job-duration-category","text":"The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below.","title":"Specify a Job Duration Category"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#test-jobs-for-expected-duration","text":"As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@opensciencegrid.org","title":"Test Jobs for Expected Duration"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#maximum-allowed-duration","text":"Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@opensciencegrid.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter.","title":"Maximum Allowed Duration"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#self-checkpointing-jobs","text":"Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@opensciencegrid.org if you have questions. :)","title":"Self-Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/","text":"Determining the Amount of Resources to Request in a Submit File \u00b6 Learning Objectives \u00b6 This guide discuses the following: - Best practices for testing jobs and scaling up your analysis. - How to determine the amount of resources (CPU, memory, disk space) to request in a submit file. Overview \u00b6 Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. If you have questions or are unsure if and how your work can be broken up, please contact us at support@opensciencegrid.org This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video: Always Start With Test Jobs \u00b6 Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up, whether this is your first time using OSG or you're an experienced user starting a new workflow. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify (and which OSG staff can help you to identify, based upon your tests). Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests. Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File \u00b6 In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and you may need to identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing (email us!). To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files. Optimize Job Resource Requests For Subsequent Jobs \u00b6 As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. Save the HTCondor log files from your jobs. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of this file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue. Submit Multiple Jobs Using A Single Submit File \u00b6 Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs. Monitoring Job Status and Obtaining Run Information \u00b6 Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: | Flag | Description | | ----------- | ----------- | | -nobatch | Combined with condor_q , this flag will list jobs individually and not by batch. | | -hold | Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. | | -run | Show your running jobs and related info, like how much time they have been running, where they are running, etc. | | -dag | Organize condor_q output by DAG. | More information about the commands and flags above can be found in the HTCondor manual . Avoid Exceeding Disk Quotas in /home and /public \u00b6 Each OSG Connect user is granted 50 GB of storage in their /home directory and 500 GB of storage in their /public directory. This may seem like a lot, but when running 100's or 1,000's of jobs, even small output can add up quickly. If these quotas are exceeded, jobs will fail or go on hold when attempting returning output. To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job. Workflow Management \u00b6 To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org .","title":"Determining the Amount of Resources to Request in a Submit File "},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#determining-the-amount-of-resources-to-request-in-a-submit-file","text":"","title":"Determining the Amount of Resources to Request in a Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#learning-objectives","text":"This guide discuses the following: - Best practices for testing jobs and scaling up your analysis. - How to determine the amount of resources (CPU, memory, disk space) to request in a submit file.","title":"Learning Objectives"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#overview","text":"Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. If you have questions or are unsure if and how your work can be broken up, please contact us at support@opensciencegrid.org This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video:","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#always-start-with-test-jobs","text":"Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up, whether this is your first time using OSG or you're an experienced user starting a new workflow. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify (and which OSG staff can help you to identify, based upon your tests). Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests.","title":"Always Start With Test Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#requesting-cpus-memory-and-disk-space-in-the-htcondor-submit-file","text":"In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and you may need to identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing (email us!). To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files.","title":"Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#optimize-job-resource-requests-for-subsequent-jobs","text":"As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. Save the HTCondor log files from your jobs. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of this file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue.","title":"Optimize Job Resource Requests For Subsequent Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#submit-multiple-jobs-using-a-single-submit-file","text":"Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs.","title":"Submit Multiple Jobs Using A Single Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#monitoring-job-status-and-obtaining-run-information","text":"Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: | Flag | Description | | ----------- | ----------- | | -nobatch | Combined with condor_q , this flag will list jobs individually and not by batch. | | -hold | Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. | | -run | Show your running jobs and related info, like how much time they have been running, where they are running, etc. | | -dag | Organize condor_q output by DAG. | More information about the commands and flags above can be found in the HTCondor manual .","title":"Monitoring Job Status and Obtaining Run Information"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#avoid-exceeding-disk-quotas-in-home-and-public","text":"Each OSG Connect user is granted 50 GB of storage in their /home directory and 500 GB of storage in their /public directory. This may seem like a lot, but when running 100's or 1,000's of jobs, even small output can add up quickly. If these quotas are exceeded, jobs will fail or go on hold when attempting returning output. To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job.","title":"Avoid Exceeding Disk Quotas in /home and /public"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#workflow-management","text":"To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool.","title":"Workflow Management"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/","text":"Roadmap to HTC Workload Submission via OSG Connect \u00b6 Overview \u00b6 This guide lays out the steps needed to go from logging in to an OSG Connect login node to running a full scale high throughput computing (HTC) workload on OSG's Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSG user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an account on the OSG Connect service and have been approved after meeting with an OSG Research Computing Facilitator. If you don't yet have an account, you can apply for one at or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below. 1. Introduction to the OSPool and OSG Connect \u00b6 The OSG's Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSG, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG that can support your HTC workload: 2. Get on OSG Connect \u00b6 After your OSG account has been approved, go through the following guides to complete your access to the login node and to enable your account to submit jobs. Generate ssh keys and login Set your default project 3. Learn to Submit HTCondor Jobs \u00b6 Computational work is run on the OSPool by submitting it as \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guides show how to submit basic HTCondor jobs. The second example allows you to see where in the OSPool your jobs run. OSG Connect Quickstart Finding OSG Locations 4. Test a First Job \u00b6 After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job -- including the software needed by the job and the appropriate mechanism to handle the data. We recommend doing this using a single test job. Prepare your software \u00b6 Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. Read through this overview of Using Software in OSG Connect to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To install your own software , begin with the guide on Compiling Software for OSG Connect and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . To use Docker containers for your jobs, start with the Docker and Singularity Containers guide , and (optionally) work through the Tensorflow tutorial (which uses Docker/Singularity) To use Distributed Environment Modules for your jobs, start with this Modules guide and then complete the Module example in this R tutorial Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSG**: Python R Machine Learning BLAST **This is not a complete list. Feel free to search for your software in our Knowledge base . Manage your data \u00b6 The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management Policies guide discussed the relevant approaches, when to use them, and where to stage data for each. Assign the Appropriate Job Duration Category \u00b6 Jobs running in the OSPool may be interrupted at any time, and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Remember that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (further below). 5. Scale Up \u00b6 After you have a sample job running successfully, you\u2019ll want to scale up in one or two steps (first run several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space you might need on the 6. Special Use Cases \u00b6 If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Build your own Docker container: Creating a Docker Container Image Submit more than 10,000 jobs at once: FAQ, search for 'max_idle' Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs Getting Help \u00b6 The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities. See our Get Help page and OSG Training page for all the different ways you can reach us. Our purpose is to assist you with achieving your computational goals, so we want to hear from you!","title":"Roadmap to HTC Workload Submission via OSG Connect "},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#roadmap-to-htc-workload-submission-via-osg-connect","text":"","title":"Roadmap to HTC Workload Submission via OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#overview","text":"This guide lays out the steps needed to go from logging in to an OSG Connect login node to running a full scale high throughput computing (HTC) workload on OSG's Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSG user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an account on the OSG Connect service and have been approved after meeting with an OSG Research Computing Facilitator. If you don't yet have an account, you can apply for one at or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#1-introduction-to-the-ospool-and-osg-connect","text":"The OSG's Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSG, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG that can support your HTC workload:","title":"1. Introduction to the OSPool and OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#2-get-on-osg-connect","text":"After your OSG account has been approved, go through the following guides to complete your access to the login node and to enable your account to submit jobs. Generate ssh keys and login Set your default project","title":"2. Get on OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#3-learn-to-submit-htcondor-jobs","text":"Computational work is run on the OSPool by submitting it as \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guides show how to submit basic HTCondor jobs. The second example allows you to see where in the OSPool your jobs run. OSG Connect Quickstart Finding OSG Locations","title":"3. Learn to Submit HTCondor Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#4-test-a-first-job","text":"After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job -- including the software needed by the job and the appropriate mechanism to handle the data. We recommend doing this using a single test job.","title":"4. Test a First Job"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#prepare-your-software","text":"Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. Read through this overview of Using Software in OSG Connect to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To install your own software , begin with the guide on Compiling Software for OSG Connect and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . To use Docker containers for your jobs, start with the Docker and Singularity Containers guide , and (optionally) work through the Tensorflow tutorial (which uses Docker/Singularity) To use Distributed Environment Modules for your jobs, start with this Modules guide and then complete the Module example in this R tutorial Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSG**: Python R Machine Learning BLAST **This is not a complete list. Feel free to search for your software in our Knowledge base .","title":"Prepare your software"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#manage-your-data","text":"The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management Policies guide discussed the relevant approaches, when to use them, and where to stage data for each.","title":"Manage your data"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#assign-the-appropriate-job-duration-category","text":"Jobs running in the OSPool may be interrupted at any time, and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Remember that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (further below).","title":"Assign the Appropriate Job Duration Category"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#5-scale-up","text":"After you have a sample job running successfully, you\u2019ll want to scale up in one or two steps (first run several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space you might need on the","title":"5. Scale Up"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#6-special-use-cases","text":"If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Build your own Docker container: Creating a Docker Container Image Submit more than 10,000 jobs at once: FAQ, search for 'max_idle' Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs","title":"6. Special Use Cases"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#getting-help","text":"The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities. See our Get Help page and OSG Training page for all the different ways you can reach us. Our purpose is to assist you with achieving your computational goals, so we want to hear from you!","title":"Getting Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/","text":"Transfer Input Files Up To 100MB In Size \u00b6 Overview \u00b6 Due to the distributed configuration of the OSG, more often than not, your jobs will need to bring along a copy (i.e. transfer a copy) of data, code, packages, software, etc. from the login node where the job is submitted to the execute node where the job will run. This requirement applies to any and all files that are needed to successfully execute and complete your job that do not otherwise exist on OSG execute servers. This guide will describe steps and important considerations for transferring input files that are <100MB in size via the HTCondor submit file. Important Considerations \u00b6 As described in the Introduction to Data Management on OSG Connect any data, files, or even software that is <100MB should be staged in your /home directory on your login node. Files in your /home directory can be transferred to jobs via your HTCondor submit file. Transfer Files From /home Using HTCondor \u00b6 To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer small file from /home transfer_input_files = my_data.csv ...other submit file details... Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Where username refers to your OSG Connect username. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Transfer Input Files Up To 100MB In Size "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#transfer-input-files-up-to-100mb-in-size","text":"","title":"Transfer Input Files Up To 100MB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#overview","text":"Due to the distributed configuration of the OSG, more often than not, your jobs will need to bring along a copy (i.e. transfer a copy) of data, code, packages, software, etc. from the login node where the job is submitted to the execute node where the job will run. This requirement applies to any and all files that are needed to successfully execute and complete your job that do not otherwise exist on OSG execute servers. This guide will describe steps and important considerations for transferring input files that are <100MB in size via the HTCondor submit file.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#important-considerations","text":"As described in the Introduction to Data Management on OSG Connect any data, files, or even software that is <100MB should be staged in your /home directory on your login node. Files in your /home directory can be transferred to jobs via your HTCondor submit file.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#transfer-files-from-home-using-htcondor","text":"To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer small file from /home transfer_input_files = my_data.csv ...other submit file details... Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Where username refers to your OSG Connect username.","title":"Transfer Files From /home Using HTCondor"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/","text":"Transfer HTTP-available Files up to 1GB In Size \u00b6 Overview \u00b6 If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address! Important Considerations \u00b6 While our Overview of Data Mangement on OSG Connect describes how you can stage data, files, or even software in OSG Connect locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for stash-based transfer the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file. Transfer Files via HTTP \u00b6 Use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer input data from home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Transfer HTTP-available Files up to 1GB In Size "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#transfer-http-available-files-up-to-1gb-in-size","text":"","title":"Transfer HTTP-available Files up to 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#overview","text":"If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address!","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#important-considerations","text":"While our Overview of Data Mangement on OSG Connect describes how you can stage data, files, or even software in OSG Connect locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for stash-based transfer the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#transfer-files-via-http","text":"Use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer input data from home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv","title":"Transfer Files via HTTP"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/","text":"==================================== Overview \u00b6 OSG Connect provides two locations for uploading files (data and software) that are needed for running jobs: home: /home/<username> (general storage from which ALL jobs should be submitted) public: /public/<username> (only for large job input and output using stash links (input) and stashcp (output)) In general, OSG Connect users are responsible for managing data in these folders and for using appropriate mechanisms for delivering data to/from jobs, as detailed below. Each is controlled with a quota and should be treated as temporary storage for active job execution. OSG Connect has no routine backup of data in these locations, and users should remove old data after jobs complete, in part, to make room for future submissions. If you think you'll need more space for a set of concurrently-queued jobs, even after cleaning up old data, please send a request to support@opensciencegrid.org ! For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ . Note: OSG Connect staff reserve the right to monitor and/or remove data without notice to the user IF doing so is necessary for ensuring proper use or to quickly fix a performance or security issue. Data Locations and Quotas \u00b6 Your OSG Connect account includes access to two data storage locations: /home and /public . Where you store your files and how your files are made accessible to your jobs depends on the size of the file and how much data is needed or produced by your jobs. Location Storage Needs Network mounted Backed Up? Initial Quota /home Storage of submit files, input files <100MB each, and per-job output up to a 1GB. Jobs should ONLY be submitted from this folder. No No 50 GB /public Staging ONLY for large input files (100MB-50GB, each) for publicly-accessible download into jobs (using stash:/// links or stashcp see below) and large output files (1-10GB). Yes No 500 GB Your quota status will be displayed when you connect to your OSG Connect login node: Disk utilization for username: /public : [ ] 0% (0/500000 MB) /home : [ # ] 4% (2147/53687 MB) You can also display your quota usage at any time using the command quota while connected to your login node. Don't hesitate to contact us at support@opensciencegrid.org if you think you need a quota increase! We can support very large amounts of data. /home Usage And Policies \u00b6 User directories within /home are meant for general-use storage of your files needed for job submission. The initial quota per user is 50 GBs, and can be increased by request to support@opensciencegrid.org when a user needs more space for appropriately-sized files. ALL JOBS MUST BE SUBMITTED FROM WITHIN /home . Users are also prohibited from making their /home directory world-readable due to security concerns. See Policies for Using OSG via OSG Connect Submit Servers for more details. If you're unable to submit jobs or your jobs are going on hold because you've reached your /home quota, please contact us at support@opensciencegrid.org about a quota increase. /public Usage and Policies \u00b6 User directories within /public are meant ONLY for staging job files too large for regular HTCondor file transfer (per-job input greater than 100MB, or per-job output greater than 1GB), and should only OSG caching mechanisms (see tables for input and output, further below). JOBS MUST NEVER BE SUBMITTED FROM WITHIN /public , and should not list /public files in the transfer_input_files or other lines of a submit file, unless as a stash:/// address (see tables further below). Files placed in /public should only be accessed by jobs using the below tools (see Transferring Data To/From Jobs ). Users violating these policies may lose the ability to submit jobs until their submissions are corrected. The initial disk quota of /public is 500 GBs. Contact support@opensciencegrid.org if you will need an increase for concurrently running work, after cleaning up all data from past jobs. Given that users should not be storing long-term data (like submit files, software, etc.) in /public , files and directories that have not been accessed for over six months may be deleted by OSG Connect staff with or without notifying the user. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone. Data is made public via the stash transfer mechanisms (which also make data public via http/https), and mirrored to a shared data repository which is available on a large number of systems around the world. Is there any support for private data? \u00b6 If you do not want your data to be downloadable by anyone, and it's small enough for HTCondor file transfer, then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files in the submit file). If it cannot be public (cannot use http or stash for job delivery), and is too large for HTCondor file transfer, then it's not a good fit for the open environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems, and our data storage locations are not backed up or suitable for project-duration storage. External Data Transfer to/from OSG Connect Login Nodes \u00b6 In general, common Unix tools such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to the OSG Connect login node, or to download files from the OSG Connect login node. Transferring Data To/From Jobs \u00b6 Transferring Input Data to Jobs \u00b6 This table summarizes the options for sending input files from the OSG Connect login node to the execution node where a job is running. This assumes that you have already uploaded these input files from your own computer to your OSG Connect login node. Transfer Method File Sizes File Location Command More Info regular HTCondor file transfer <100 MB; <500 MB total per job /home transfer_input_files HTCondor File Transfer OSG's transfer tools >1GB; <10 GB per job /public stash address in transfer_input_files Stash Transfer HTTP <1GB non-OSG web server http address in transfer_input_files HTTP Access GridFTP > 10 GB /public or non-OSG data server gfal-copy contact us Transferring Output Data from Jobs \u00b6 This table summarizes a job's options for returning output files generated by the job back to the OSG Connect login node. Transfer Method File Sizes Transfer To Command More Info regular HTCondor file transfer < 1 GB /home HTCondor default output transfer or transfer_output_files HTCondor Transfer OSG's transfer tools >1GB and < 10 GB /public stashcp in job executable stashcp GridFTP > 10 GB /public gfal-copy contact us Watch this video from the 2021 OSG Virtual School for more information about Handling Data on OSG: Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Overview: Data Staging and Transfer to Jobs "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#overview","text":"OSG Connect provides two locations for uploading files (data and software) that are needed for running jobs: home: /home/<username> (general storage from which ALL jobs should be submitted) public: /public/<username> (only for large job input and output using stash links (input) and stashcp (output)) In general, OSG Connect users are responsible for managing data in these folders and for using appropriate mechanisms for delivering data to/from jobs, as detailed below. Each is controlled with a quota and should be treated as temporary storage for active job execution. OSG Connect has no routine backup of data in these locations, and users should remove old data after jobs complete, in part, to make room for future submissions. If you think you'll need more space for a set of concurrently-queued jobs, even after cleaning up old data, please send a request to support@opensciencegrid.org ! For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ . Note: OSG Connect staff reserve the right to monitor and/or remove data without notice to the user IF doing so is necessary for ensuring proper use or to quickly fix a performance or security issue.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#data-locations-and-quotas","text":"Your OSG Connect account includes access to two data storage locations: /home and /public . Where you store your files and how your files are made accessible to your jobs depends on the size of the file and how much data is needed or produced by your jobs. Location Storage Needs Network mounted Backed Up? Initial Quota /home Storage of submit files, input files <100MB each, and per-job output up to a 1GB. Jobs should ONLY be submitted from this folder. No No 50 GB /public Staging ONLY for large input files (100MB-50GB, each) for publicly-accessible download into jobs (using stash:/// links or stashcp see below) and large output files (1-10GB). Yes No 500 GB Your quota status will be displayed when you connect to your OSG Connect login node: Disk utilization for username: /public : [ ] 0% (0/500000 MB) /home : [ # ] 4% (2147/53687 MB) You can also display your quota usage at any time using the command quota while connected to your login node. Don't hesitate to contact us at support@opensciencegrid.org if you think you need a quota increase! We can support very large amounts of data.","title":"Data Locations and Quotas"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#home-usage-and-policies","text":"User directories within /home are meant for general-use storage of your files needed for job submission. The initial quota per user is 50 GBs, and can be increased by request to support@opensciencegrid.org when a user needs more space for appropriately-sized files. ALL JOBS MUST BE SUBMITTED FROM WITHIN /home . Users are also prohibited from making their /home directory world-readable due to security concerns. See Policies for Using OSG via OSG Connect Submit Servers for more details. If you're unable to submit jobs or your jobs are going on hold because you've reached your /home quota, please contact us at support@opensciencegrid.org about a quota increase.","title":"/home Usage And Policies"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#public-usage-and-policies","text":"User directories within /public are meant ONLY for staging job files too large for regular HTCondor file transfer (per-job input greater than 100MB, or per-job output greater than 1GB), and should only OSG caching mechanisms (see tables for input and output, further below). JOBS MUST NEVER BE SUBMITTED FROM WITHIN /public , and should not list /public files in the transfer_input_files or other lines of a submit file, unless as a stash:/// address (see tables further below). Files placed in /public should only be accessed by jobs using the below tools (see Transferring Data To/From Jobs ). Users violating these policies may lose the ability to submit jobs until their submissions are corrected. The initial disk quota of /public is 500 GBs. Contact support@opensciencegrid.org if you will need an increase for concurrently running work, after cleaning up all data from past jobs. Given that users should not be storing long-term data (like submit files, software, etc.) in /public , files and directories that have not been accessed for over six months may be deleted by OSG Connect staff with or without notifying the user. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone. Data is made public via the stash transfer mechanisms (which also make data public via http/https), and mirrored to a shared data repository which is available on a large number of systems around the world.","title":"/public Usage and Policies"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#is-there-any-support-for-private-data","text":"If you do not want your data to be downloadable by anyone, and it's small enough for HTCondor file transfer, then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files in the submit file). If it cannot be public (cannot use http or stash for job delivery), and is too large for HTCondor file transfer, then it's not a good fit for the open environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems, and our data storage locations are not backed up or suitable for project-duration storage.","title":"Is there any support for private data?"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#external-data-transfer-tofrom-osg-connect-login-nodes","text":"In general, common Unix tools such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to the OSG Connect login node, or to download files from the OSG Connect login node.","title":"External Data Transfer to/from OSG Connect Login Nodes"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-data-tofrom-jobs","text":"","title":"Transferring Data To/From Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-input-data-to-jobs","text":"This table summarizes the options for sending input files from the OSG Connect login node to the execution node where a job is running. This assumes that you have already uploaded these input files from your own computer to your OSG Connect login node. Transfer Method File Sizes File Location Command More Info regular HTCondor file transfer <100 MB; <500 MB total per job /home transfer_input_files HTCondor File Transfer OSG's transfer tools >1GB; <10 GB per job /public stash address in transfer_input_files Stash Transfer HTTP <1GB non-OSG web server http address in transfer_input_files HTTP Access GridFTP > 10 GB /public or non-OSG data server gfal-copy contact us","title":"Transferring Input Data to Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-output-data-from-jobs","text":"This table summarizes a job's options for returning output files generated by the job back to the OSG Connect login node. Transfer Method File Sizes Transfer To Command More Info regular HTCondor file transfer < 1 GB /home HTCondor default output transfer or transfer_output_files HTCondor Transfer OSG's transfer tools >1GB and < 10 GB /public stashcp in job executable stashcp GridFTP > 10 GB /public gfal-copy contact us Watch this video from the 2021 OSG Virtual School for more information about Handling Data on OSG:","title":"Transferring Output Data from Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/","text":"Transfer Job Output Files Up To 1GB In Size \u00b6 Overview \u00b6 When your OSG Connect jobs run, any output that gets generated is specifically written to the execute node on which the job ran. In order to get access to your output files, a copy of the output must be transferred back to your OSG Connect login node. This guide will describe the necessary steps, along with important considerations, for transferring your output files back to your /home directory on your OSG Connect login node. Important Considerations \u00b6 As described in the Introduction to Data Management on OSG Connect , any output <1GB should be staged in your /home directory. For output files >1GB, please refer to our Transfer Large Input and Output Files >1GB In Size guide. If your jobs use any input files >1GB that are transferred from your /public directory using StacheCash, it is important that these files get deleted from the job's working directory or moved to a subdirectory so that HTCondor will not transfer these large files back to your /home directory. Use HTCondor To Transfer Output <1GB \u00b6 By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide. To learn more, please contact us at support@opensciencegrid.org . What is the top-level directory of a job? Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted. What if my output file(s) are not written to the top-level directory? If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : 1 2 3 4 5 6 #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR Group Multiple Output Files For Convenience \u00b6 If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: 1 2 3 4 5 6 7 8 #! /bin/bash # various commands needed to run your job # create output tar archive mkidr my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory. Select Specific Output Files To Transfer to /home Using HTCondor \u00b6 As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, we recommend deleting the unrequired output files or moving them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory. In cases where a bash script is not used as the excutable of your job and you wish to have only specific output files transferred back, please contact us at support@opensciencegrid.org . Get Additional Options For Managing Job Output \u00b6 Several options exist for managing output file transfers back to your /home directory and we encourage you to get in touch with us at support@opensciencegrid.org to help identify the best solution for your needs.","title":"Transfer Job Output Files Up To 1GB In Size "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#transfer-job-output-files-up-to-1gb-in-size","text":"","title":"Transfer Job Output Files Up To 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#overview","text":"When your OSG Connect jobs run, any output that gets generated is specifically written to the execute node on which the job ran. In order to get access to your output files, a copy of the output must be transferred back to your OSG Connect login node. This guide will describe the necessary steps, along with important considerations, for transferring your output files back to your /home directory on your OSG Connect login node.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#important-considerations","text":"As described in the Introduction to Data Management on OSG Connect , any output <1GB should be staged in your /home directory. For output files >1GB, please refer to our Transfer Large Input and Output Files >1GB In Size guide. If your jobs use any input files >1GB that are transferred from your /public directory using StacheCash, it is important that these files get deleted from the job's working directory or moved to a subdirectory so that HTCondor will not transfer these large files back to your /home directory.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#use-htcondor-to-transfer-output-1gb","text":"By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide. To learn more, please contact us at support@opensciencegrid.org . What is the top-level directory of a job? Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted. What if my output file(s) are not written to the top-level directory? If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : 1 2 3 4 5 6 #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR","title":"Use HTCondor To Transfer Output &lt;1GB"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#group-multiple-output-files-for-convenience","text":"If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: 1 2 3 4 5 6 7 8 #! /bin/bash # various commands needed to run your job # create output tar archive mkidr my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory.","title":"Group Multiple Output Files For Convenience"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#select-specific-output-files-to-transfer-to-home-using-htcondor","text":"As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, we recommend deleting the unrequired output files or moving them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory. In cases where a bash script is not used as the excutable of your job and you wish to have only specific output files transferred back, please contact us at support@opensciencegrid.org .","title":"Select Specific Output Files To Transfer to /home Using HTCondor"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#get-additional-options-for-managing-job-output","text":"Several options exist for managing output file transfers back to your /home directory and we encourage you to get in touch with us at support@opensciencegrid.org to help identify the best solution for your needs.","title":"Get Additional Options For Managing Job Output"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/","text":"Use scp To Transfer Files To and From OSG Connect \u00b6 Overview \u00b6 This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the OSG Connect login node using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Connect login node) are indicated using username@machine:/path/to/file . Transfer Files To OSG Connect \u00b6 Let's say you have a file you wish to transfer to OSG Connect named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on OSG Connect. Note that you will not be logged into OSG Connect when you perform this step. $ scp my_file.txt username@loginNN.osgconnect.net:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@loginNN.osgconnect.net:/public/username/ Transfer Directories To OSG Connect \u00b6 To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@loginNN.osgconnect.net:/home/username/ Transfer Files From OSG Connect \u00b6 To transfer files from OSG Connect back to your laptop or desktop you can use the scp as shown above, but with the source being the copy that is located on OSG Connect: $ scp username@loginNN.osgconnect.net:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer Transfer Files Between OSG Connect and Another Server \u00b6 scp can be used to transfer files between OSG Connect and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server and your OSG Connect login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@loginNN.osgconnect.net:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Use scp To Transfer Files To and From OSG Connect "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#use-scp-to-transfer-files-to-and-from-osg-connect","text":"","title":"Use scp To Transfer Files To and From OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#overview","text":"This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the OSG Connect login node using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Connect login node) are indicated using username@machine:/path/to/file .","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-to-osg-connect","text":"Let's say you have a file you wish to transfer to OSG Connect named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on OSG Connect. Note that you will not be logged into OSG Connect when you perform this step. $ scp my_file.txt username@loginNN.osgconnect.net:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@loginNN.osgconnect.net:/public/username/","title":"Transfer Files To OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-directories-to-osg-connect","text":"To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@loginNN.osgconnect.net:/home/username/","title":"Transfer Directories To OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-from-osg-connect","text":"To transfer files from OSG Connect back to your laptop or desktop you can use the scp as shown above, but with the source being the copy that is located on OSG Connect: $ scp username@loginNN.osgconnect.net:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer","title":"Transfer Files From OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-between-osg-connect-and-another-server","text":"scp can be used to transfer files between OSG Connect and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server and your OSG Connect login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@loginNN.osgconnect.net:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file.","title":"Transfer Files Between OSG Connect and Another Server"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/","text":"Transfer Larger Input and Output Files \u00b6 Overview \u00b6 For input files >100MB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the login nodes and their network capacity. And this is exactly why the OSG Data Federation exists for researchers with larger per-job data! Users on an OSG Connect login node can handle such files via the OSG Connect data caching origin (mounted and visible as the /public location) and use OSG's caching tools to scalably transfer them between the running jobs and the origin. The OSG caching tools ensure faster delivery to and from execute nodes by taking adantage of regional data caches in the OSG Data Federation, while preserving login node performance. Important Considerations and Best Practices \u00b6 As described in OSG Connect's Introduction to Data Management on OSG Connect , the /public location must be used for: Any input data or software larger than 100MB for transfer to jobs using OSG caching tools Any per-job output >1GB and <10GB , which should ONLY be transferred back to the origin using a stashcp command within the job executable. User must never submit jobs from the /public location, and should continue to ONLY submit jobs from within their /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory, unless otherwise directed by an OSG staff member. Thus, files within the /public location should only be referenced within the submit file by using the methods described further below , and should never be listed for direct HTCondor transfer via transfer_input_files , transfer_output_files , or transfer_output_remaps . The /public location is a mount of the OSG Connect origin filesystem. It is mounted to the OSG Connect login nodes only so that users can appropriately stage large job inputs or retrieve outputs via the login nodes. Because of impacts to the filesystem of the data origin, files in the data origin ( /public ) should be organized in one or very few files per job. The filesystem is likely to encounter performance issues if/when files accumulated there are highly numerous and/or small. The /public location is otherwise unnecessary for smaller files (which can and should be served via the user's /home directory and regular HTCondor file transfer). Smaller files should only be handled via /public with explicit instruction from an OSG staff member. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone, via the web. Data is made public via stash transfer (and, thus, via http addresses), and mirrored to a shared data repository which is available on a large number of systems around the world. Use a 'stash' URL to Transfer Large Input Files to Jobs \u00b6 Jobs submitted from the OSG Connect login nodes will transfer data from the origin when files are indicated with an appropriate stash:/// URL in the transfer_input_files line of the submit file: Upload your larger input and/or software files to your /public directory which is accessible via your OSG Connect login node at /public/username for which our Using scp To Transfer Files To OSG Connect guide may be helpful. Because of the way your files in /public are cached across the Open Science Pool, any changes or modifications that you make after placing a file in /public will not be propagated. This means that if you add a new version of a file to your /public directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files in /public . Add the necessary details to your HTCondor submit file to tell HTCondor which files to transfer, and that your jobs must run on executes nodes that have access to the Open Science Data Federation. # Submit file example of large input/software transfer log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out #Transfer input files transfer_input_files = stash:///osgconnect/public/<username>/<dir>/<filename>, <other files> ...other submit file details... Note how the /public mount (visible on the login node) corresponds to the /osgconnect/public namespace across the Open Science Federation. For example, if the data file is located at /public/<username>/samples/sample01.dat , then the stash:/// URL to transfer this file to the job's working directory on the execute point would be: stash:///osgconnect/public/<username>/samples/sample01.dat Use stashcp to Transfer Larger Job Outputs to the Data Origin \u00b6 For output, users should use the stashcp command within their job executable, which will transfer the user's specified file to the specific location in the data origin. Remember that you should NEVER list a /public location within the submit file (e.g. in 'transfer_output_remaps ) or submit jobs from within /public` . Add the necessary details to your HTCondor submit file to tell HTCondor that your jobs must run on executes nodes that have access to the stashcp module (among other -supported modules). Note that the output files are NOT listed anywhere in the submit file for transfer purposes. # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") && (HAS_MODULES =?= true) ...other submit file details... Add a stashcp command at the end of your executable to transfer the data files back to the OSG Connect data origin (within /public ). You will need to prepend your /public directory path with stash:///osgconnect as follows: 1 2 3 4 5 6 #!/bin/bash # other commands to be executed in job: # transfer large output to public stashcp <filename> stash:///osgconnect/public/username/path/<filename> For example, if you wish to transfer output.dat to the directory /public/<username>/output/ then the stash command would be: stashcp output.dat stash:///osgconnect/public/<username>/output/output.dat Note that the output file name must also be included at the end of the /public path where the file will be transferred, which also allows you to rename the file. Stachcp Command Manual \u00b6 More usage options are described in the stashcp help message: $ stashcp -h Usage: stashcp [options] source destination Options: -h, --help show this help message and exit -d, --debug debug -r recursively copy --closest Return the closest cache and exit -c CACHE, --cache=CACHE Cache to use -j CACHES_JSON, --caches-json=CACHES_JSON The JSON file containing the list of caches --methods=METHODS Comma separated list of methods to try, in order. Default: cvmfs,xrootd,http -t TOKEN, --token=TOKEN Token file to use for reading and/or writing Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Transfer Larger Input and Output Files "},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#transfer-larger-input-and-output-files","text":"","title":"Transfer Larger Input and Output Files"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#overview","text":"For input files >100MB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the login nodes and their network capacity. And this is exactly why the OSG Data Federation exists for researchers with larger per-job data! Users on an OSG Connect login node can handle such files via the OSG Connect data caching origin (mounted and visible as the /public location) and use OSG's caching tools to scalably transfer them between the running jobs and the origin. The OSG caching tools ensure faster delivery to and from execute nodes by taking adantage of regional data caches in the OSG Data Federation, while preserving login node performance.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#important-considerations-and-best-practices","text":"As described in OSG Connect's Introduction to Data Management on OSG Connect , the /public location must be used for: Any input data or software larger than 100MB for transfer to jobs using OSG caching tools Any per-job output >1GB and <10GB , which should ONLY be transferred back to the origin using a stashcp command within the job executable. User must never submit jobs from the /public location, and should continue to ONLY submit jobs from within their /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory, unless otherwise directed by an OSG staff member. Thus, files within the /public location should only be referenced within the submit file by using the methods described further below , and should never be listed for direct HTCondor transfer via transfer_input_files , transfer_output_files , or transfer_output_remaps . The /public location is a mount of the OSG Connect origin filesystem. It is mounted to the OSG Connect login nodes only so that users can appropriately stage large job inputs or retrieve outputs via the login nodes. Because of impacts to the filesystem of the data origin, files in the data origin ( /public ) should be organized in one or very few files per job. The filesystem is likely to encounter performance issues if/when files accumulated there are highly numerous and/or small. The /public location is otherwise unnecessary for smaller files (which can and should be served via the user's /home directory and regular HTCondor file transfer). Smaller files should only be handled via /public with explicit instruction from an OSG staff member. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone, via the web. Data is made public via stash transfer (and, thus, via http addresses), and mirrored to a shared data repository which is available on a large number of systems around the world.","title":"Important Considerations and Best Practices"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#use-a-stash-url-to-transfer-large-input-files-to-jobs","text":"Jobs submitted from the OSG Connect login nodes will transfer data from the origin when files are indicated with an appropriate stash:/// URL in the transfer_input_files line of the submit file: Upload your larger input and/or software files to your /public directory which is accessible via your OSG Connect login node at /public/username for which our Using scp To Transfer Files To OSG Connect guide may be helpful. Because of the way your files in /public are cached across the Open Science Pool, any changes or modifications that you make after placing a file in /public will not be propagated. This means that if you add a new version of a file to your /public directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files in /public . Add the necessary details to your HTCondor submit file to tell HTCondor which files to transfer, and that your jobs must run on executes nodes that have access to the Open Science Data Federation. # Submit file example of large input/software transfer log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out #Transfer input files transfer_input_files = stash:///osgconnect/public/<username>/<dir>/<filename>, <other files> ...other submit file details... Note how the /public mount (visible on the login node) corresponds to the /osgconnect/public namespace across the Open Science Federation. For example, if the data file is located at /public/<username>/samples/sample01.dat , then the stash:/// URL to transfer this file to the job's working directory on the execute point would be: stash:///osgconnect/public/<username>/samples/sample01.dat","title":"Use a 'stash' URL to Transfer Large Input Files to Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#use-stashcp-to-transfer-larger-job-outputs-to-the-data-origin","text":"For output, users should use the stashcp command within their job executable, which will transfer the user's specified file to the specific location in the data origin. Remember that you should NEVER list a /public location within the submit file (e.g. in 'transfer_output_remaps ) or submit jobs from within /public` . Add the necessary details to your HTCondor submit file to tell HTCondor that your jobs must run on executes nodes that have access to the stashcp module (among other -supported modules). Note that the output files are NOT listed anywhere in the submit file for transfer purposes. # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") && (HAS_MODULES =?= true) ...other submit file details... Add a stashcp command at the end of your executable to transfer the data files back to the OSG Connect data origin (within /public ). You will need to prepend your /public directory path with stash:///osgconnect as follows: 1 2 3 4 5 6 #!/bin/bash # other commands to be executed in job: # transfer large output to public stashcp <filename> stash:///osgconnect/public/username/path/<filename> For example, if you wish to transfer output.dat to the directory /public/<username>/output/ then the stash command would be: stashcp output.dat stash:///osgconnect/public/<username>/output/output.dat Note that the output file name must also be included at the end of the /public path where the file will be transferred, which also allows you to rename the file.","title":"Use stashcp to Transfer Larger Job Outputs to the Data Origin"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#stachcp-command-manual","text":"More usage options are described in the stashcp help message: $ stashcp -h Usage: stashcp [options] source destination Options: -h, --help show this help message and exit -d, --debug debug -r recursively copy --closest Return the closest cache and exit -c CACHE, --cache=CACHE Cache to use -j CACHES_JSON, --caches-json=CACHES_JSON The JSON file containing the list of caches --methods=METHODS Comma separated list of methods to try, in order. Default: cvmfs,xrootd,http -t TOKEN, --token=TOKEN Token file to use for reading and/or writing","title":"Stachcp Command Manual"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/","text":"Checkpointing Jobs \u00b6 What is Checkpointing? \u00b6 Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint. Why Checkpoint? \u00b6 Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit. Process of Exit Driven Checkpointing \u00b6 Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs. Requirements for Exit Driven Checkpointing \u00b6 Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing. Changes to the Submit File \u00b6 Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1 Example Wrapper Script for Checkpointing Job \u00b6 As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher. Checking the Progress of Checkpointing Jobs \u00b6 It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSG Connect login node. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 . More Information \u00b6 More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#checkpointing-jobs","text":"","title":"Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#what-is-checkpointing","text":"Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint.","title":"What is Checkpointing?"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#why-checkpoint","text":"Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit.","title":"Why Checkpoint?"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#process-of-exit-driven-checkpointing","text":"Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs.","title":"Process of Exit Driven Checkpointing"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#requirements-for-exit-driven-checkpointing","text":"Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing.","title":"Requirements for Exit Driven Checkpointing"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#changes-to-the-submit-file","text":"Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1","title":"Changes to the Submit File"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#example-wrapper-script-for-checkpointing-job","text":"As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher.","title":"Example Wrapper Script for Checkpointing Job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#checking-the-progress-of-checkpointing-jobs","text":"It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSG Connect login node. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 .","title":"Checking the Progress of Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#more-information","text":"More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"More Information"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/","text":"Easily Submit Multiple Jobs \u00b6 Overview \u00b6 HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSG Connect submit server , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Connect support and we will work with you to identify options to meet the needs of your work. Submit Multiple Jobs Using queue \u00b6 All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc. 1. Use queue N in you HTCondor submit files \u00b6 When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include: A. Use integer numbered input files \u00b6 [user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100 B. Specify a row or column number for each job \u00b6 $(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software. C. Need N to start at 1 \u00b6 If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables. 2. Submit multiple jobs with one or more distinct variables per job \u00b6 Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial . Use multiple variables for each job \u00b6 Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt 3. Organizing Jobs Into Individual Directories \u00b6 One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Easily Submit Multiple Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#easily-submit-multiple-jobs","text":"","title":"Easily Submit Multiple Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#overview","text":"HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSG Connect submit server , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Connect support and we will work with you to identify options to meet the needs of your work.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#submit-multiple-jobs-using-queue","text":"All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc.","title":"Submit Multiple Jobs Using queue"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#1-use-queue-n-in-you-htcondor-submit-files","text":"When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include:","title":"1. Use queue N in you HTCondor submit files"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#a-use-integer-numbered-input-files","text":"[user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100","title":"A. Use integer numbered input files"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#b-specify-a-row-or-column-number-for-each-job","text":"$(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software.","title":"B. Specify a row or column number for each job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#c-need-n-to-start-at-1","text":"If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables.","title":"C. Need N to start at 1"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#2-submit-multiple-jobs-with-one-or-more-distinct-variables-per-job","text":"Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial .","title":"2. Submit multiple jobs with one or more distinct variables per job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#use-multiple-variables-for-each-job","text":"Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt","title":"Use multiple variables for each job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#3-organizing-jobs-into-individual-directories","text":"One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location.","title":"3. Organizing Jobs Into Individual Directories"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/","text":"Use OSG Connect Tutorials \u00b6 OSG Connect tutorials on Github \u00b6 All of the OSG Connect tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us. Tutorial commands \u00b6 From the OSG Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name> Available tutorials \u00b6 The OSG Connect login nodes have the following tutorials pre-installed. To see what is available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial stash-cvmfs ............ Shows how to use stash-cvmfs for input data transfer stash-http ............. Retrieve job input files from Stash via HTTP tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Install and setup a tutorial \u00b6 On the OSG Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Use OSG Connect Tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#use-osg-connect-tutorials","text":"","title":"Use OSG Connect Tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#osg-connect-tutorials-on-github","text":"All of the OSG Connect tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us.","title":"OSG Connect tutorials on Github"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#tutorial-commands","text":"From the OSG Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name>","title":"Tutorial commands"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#available-tutorials","text":"The OSG Connect login nodes have the following tutorials pre-installed. To see what is available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial stash-cvmfs ............ Shows how to use stash-cvmfs for input data transfer stash-http ............. Retrieve job input files from Stash via HTTP tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication","title":"Available tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#install-and-setup-a-tutorial","text":"On the OSG Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Install and setup a tutorial"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/","text":"View Existing OSPool-Supported Containers \u00b6 This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on container overview and creating containers for detailed instructions on how to use containers and/or have Docker containers added to the OSPool's approved list. Also note that this list is not complete. There are many images under /cvmfs/singularity.opensciencegrid.org/ which are either project specific or not described well enough to make this list. Base \u00b6 Name CVMFS Locations Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest Enterprise Linux (CentOS) 6 base image Project Website Container Definition EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest Enterprise Linux (CentOS) 7 base image Project Website Container Definition EL 8 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest Enterprise Linux (CentOS) 8 base image Project Website Container Definition Ubuntu 16.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Ubuntu 16.04 (Xenial) base image Project Website Container Definition Ubuntu 18.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-18.04:latest Ubuntu 18.04 (Bionic) base image Project Website Container Definition Ubuntu 20.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Ubuntu 20.04 (Focal) base image Project Website Container Definition Languages \u00b6 Name CVMFS Locations Description Julia /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Ubuntu based image with Julia Project Website Container Definition Matlab Runtime /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b This is the Matlab runtime component you can use to execute compiled Matlab codes Project Website Container Definition R /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 Example for building R images Project Website Container Definition Project \u00b6 Name CVMFS Locations Description XENONnT /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:development /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latest /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:development /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latest /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:development /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition Tools \u00b6 Name CVMFS Locations Description FreeSurfer /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies Project Website Container Definition GROMACS /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. Project Website Container Definition GROMACS GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. Project Website Container Definition Quantum Espresso /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 A suite for first-principles electronic-structure calculations and materials modeling Project Website Container Definition TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest TensorFlow image (CPU only) Project Website Container Definition TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest TensorFlow image with GPU support Project Website Container Definition","title":"View Existing OSPool-Supported Containers "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#view-existing-ospool-supported-containers","text":"This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on container overview and creating containers for detailed instructions on how to use containers and/or have Docker containers added to the OSPool's approved list. Also note that this list is not complete. There are many images under /cvmfs/singularity.opensciencegrid.org/ which are either project specific or not described well enough to make this list.","title":"View Existing OSPool-Supported Containers"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#base","text":"Name CVMFS Locations Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest Enterprise Linux (CentOS) 6 base image Project Website Container Definition EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest Enterprise Linux (CentOS) 7 base image Project Website Container Definition EL 8 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest Enterprise Linux (CentOS) 8 base image Project Website Container Definition Ubuntu 16.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Ubuntu 16.04 (Xenial) base image Project Website Container Definition Ubuntu 18.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-18.04:latest Ubuntu 18.04 (Bionic) base image Project Website Container Definition Ubuntu 20.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Ubuntu 20.04 (Focal) base image Project Website Container Definition","title":"Base"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#languages","text":"Name CVMFS Locations Description Julia /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Ubuntu based image with Julia Project Website Container Definition Matlab Runtime /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b This is the Matlab runtime component you can use to execute compiled Matlab codes Project Website Container Definition R /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 Example for building R images Project Website Container Definition","title":"Languages"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#project","text":"Name CVMFS Locations Description XENONnT /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:development /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latest /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:development /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latest /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:development /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition","title":"Project"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#tools","text":"Name CVMFS Locations Description FreeSurfer /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies Project Website Container Definition GROMACS /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. Project Website Container Definition GROMACS GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. Project Website Container Definition Quantum Espresso /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 A suite for first-principles electronic-structure calculations and materials modeling Project Website Container Definition TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest TensorFlow image (CPU only) Project Website Container Definition TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest TensorFlow image with GPU support Project Website Container Definition","title":"Tools"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/","text":"Compiling Software for OSG Connect \u00b6 Compiling Software for OSG Connect \u00b6 Introduction \u00b6 Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides useful information for compiling and using your software in OSG Connect. A detailed example of performing software compilation is additionally available at OSG Connect Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program. Get software source code \u00b6 The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software. Select the appropriate compiler and compilation options \u00b6 A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the login nodes have GCC 4.8.5 as the default version, but newer versions of GCC may also be available - to learn more please contact support@opensciencegrid.org . CMake is a commonly used compilation platform. Your software may have dependencies for specific cmake versions. Currently the login nodes have two versions of CMake, 3.12.3 and 3.13.0 available as modules . Static versus dynamic linking during compilation \u00b6 Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSG Connect users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ). Get access to libraries needed for your software \u00b6 As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software. The distributed environment modules system available on OSG Connect also provides several commonly used software libraries (lapack, atlas, hdf5, netcdf, etc.) that can be used for your software compilation and execution. The appropriate modules should be loaded before performing your software compilation. The process of loading a module will modify all appropriate environment variables (e.g. CPATH and LIBRARY_PATH ) for you. If you do use modules from the modules system, you will need to modify your job scripts to load the appropriate modules before running you software. Perform your compilation \u00b6 Software compilation is easiest to perform interactively, and OSG Connect users are welcome to compile software directly on their assigned login node. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Connect login nodes currently use the Red Hat Enterprise Linux 7 operating system, your software will, generally, only be compatible for execution on RHEL 7 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 7\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Connect account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG Connect username and path replaced with the path to the directory you created for your software installation. Use Your Software \u00b6 When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG Connect guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software. Get Additional Assistance \u00b6 If you have questions or need assistance, please contact support@opensciencegrid.org .","title":"Compiling Software for OSG Connect "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#compiling-software-for-osg-connect","text":"","title":"Compiling Software for OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#compiling-software-for-osg-connect_1","text":"","title":"Compiling Software for OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#introduction","text":"Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides useful information for compiling and using your software in OSG Connect. A detailed example of performing software compilation is additionally available at OSG Connect Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program.","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-software-source-code","text":"The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software.","title":"Get software source code"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#select-the-appropriate-compiler-and-compilation-options","text":"A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the login nodes have GCC 4.8.5 as the default version, but newer versions of GCC may also be available - to learn more please contact support@opensciencegrid.org . CMake is a commonly used compilation platform. Your software may have dependencies for specific cmake versions. Currently the login nodes have two versions of CMake, 3.12.3 and 3.13.0 available as modules .","title":"Select the appropriate compiler and compilation options"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#static-versus-dynamic-linking-during-compilation","text":"Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSG Connect users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ).","title":"Static versus dynamic linking during compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-access-to-libraries-needed-for-your-software","text":"As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software. The distributed environment modules system available on OSG Connect also provides several commonly used software libraries (lapack, atlas, hdf5, netcdf, etc.) that can be used for your software compilation and execution. The appropriate modules should be loaded before performing your software compilation. The process of loading a module will modify all appropriate environment variables (e.g. CPATH and LIBRARY_PATH ) for you. If you do use modules from the modules system, you will need to modify your job scripts to load the appropriate modules before running you software.","title":"Get access to libraries needed for your software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#perform-your-compilation","text":"Software compilation is easiest to perform interactively, and OSG Connect users are welcome to compile software directly on their assigned login node. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Connect login nodes currently use the Red Hat Enterprise Linux 7 operating system, your software will, generally, only be compatible for execution on RHEL 7 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 7\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Connect account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG Connect username and path replaced with the path to the directory you created for your software installation.","title":"Perform your compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#use-your-software","text":"When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG Connect guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software.","title":"Use Your Software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-additional-assistance","text":"If you have questions or need assistance, please contact support@opensciencegrid.org .","title":"Get Additional Assistance"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/","text":"Create/Register a Docker Container Image \u00b6 This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container). Once you have created your custom image, you will need to register the image as described further down in this guide. For an overview and how to execute images on OSG, please see Use Containers on the OSG Install Docker and Get a Docker Hub Account \u00b6 You'll need a Docker Hub account in order to download Docker and share your Docker container images with the OSG: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. If you prefer, you can base your image on images not already published by OSG, but if you do this, we recommend that as one of the creation steps you create the /cvmfs directory. See Special Cases below. Build a Container \u00b6 There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs. Editing the Dockerfile \u00b6 Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast . Editing the default image using local Docker \u00b6 You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name. Upload Docker Container to Docker Hub \u00b6 Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, return to our Containers in OSG Guide to learn how to upload your container to the OSG's container repository. Submit your Docker Container to the OSG Repository \u00b6 Once your Docker image has been published on Docker Hub, it needs to be submitted to the OSG Singularity repository ( /cvmfs/singularity.opensciencegrid.org/ ), which also hosts the OSG-provided default images. To get your images included, please create a git pull request with the container identifier in docker_images.txt in the cvmfs-singularity-sync repository , or contact support@opensciencegrid.org and we can help you. Once your submission has been accepted, it will be automatically converted to a Singularity image and pushed to the OSG Singularity repository (see further above). Note: some common Dockerfile features, like ENV and ENTRYPOINT, are ignored when the Docker image is converted to a Singularity image. Once your container has been added to CVMFS, if you update your original Docker image, new versions pushed to Docker Hub will automatically be detected and the version on the OSG (in the CVMFS filesystem) will be updated accordingly. Special Cases \u00b6 Accessing CVMFS \u00b6 If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file. ENTRYPOINT and ENV \u00b6 Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool. Email us if you would like to preserve these attributes.","title":"Create/Register a Docker Container Image "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#createregister-a-docker-container-image","text":"This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container). Once you have created your custom image, you will need to register the image as described further down in this guide. For an overview and how to execute images on OSG, please see Use Containers on the OSG","title":"Create/Register a Docker Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#install-docker-and-get-a-docker-hub-account","text":"You'll need a Docker Hub account in order to download Docker and share your Docker container images with the OSG: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts.","title":"Install Docker and Get a Docker Hub Account"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. If you prefer, you can base your image on images not already published by OSG, but if you do this, we recommend that as one of the creation steps you create the /cvmfs directory. See Special Cases below.","title":"Identify Components"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#build-a-container","text":"There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs.","title":"Build a Container"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#editing-the-dockerfile","text":"Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast .","title":"Editing the Dockerfile"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#editing-the-default-image-using-local-docker","text":"You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name.","title":"Editing the default image using local Docker"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#upload-docker-container-to-docker-hub","text":"Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, return to our Containers in OSG Guide to learn how to upload your container to the OSG's container repository.","title":"Upload Docker Container to Docker Hub"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#submit-your-docker-container-to-the-osg-repository","text":"Once your Docker image has been published on Docker Hub, it needs to be submitted to the OSG Singularity repository ( /cvmfs/singularity.opensciencegrid.org/ ), which also hosts the OSG-provided default images. To get your images included, please create a git pull request with the container identifier in docker_images.txt in the cvmfs-singularity-sync repository , or contact support@opensciencegrid.org and we can help you. Once your submission has been accepted, it will be automatically converted to a Singularity image and pushed to the OSG Singularity repository (see further above). Note: some common Dockerfile features, like ENV and ENTRYPOINT, are ignored when the Docker image is converted to a Singularity image. Once your container has been added to CVMFS, if you update your original Docker image, new versions pushed to Docker Hub will automatically be detected and the version on the OSG (in the CVMFS filesystem) will be updated accordingly.","title":"Submit your Docker Container to the OSG Repository"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#special-cases","text":"","title":"Special Cases"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#accessing-cvmfs","text":"If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file.","title":"Accessing CVMFS"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#entrypoint-and-env","text":"Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool. Email us if you would like to preserve these attributes.","title":"ENTRYPOINT and ENV"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/","text":"Create a Singularity Container Image \u00b6 NOTE: Building Singularity containers are currently not supported on the OSGConnect access point. The guide assumes that you have your own Linux machine where you can install Singularity and execute commands via sudo. This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on OSG, please see Use Containers on the OSG Install Singularity \u00b6 Install Singularity to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. sudo is required to build the images. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. Editing the Build Spec \u00b6 Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install See the Singularity documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install Once your build spec is ready, you can \"build\" the container image by running this command: $ sudo singularity build my-container.sif image.def Note that sudo here is currently required. Once the image is built, you can upload it to Stash, test it on OSGConenct, and use it in your HTCondor jobs. This is all described in the container guide .","title":"Create a Singularity Container Image "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#create-a-singularity-container-image","text":"NOTE: Building Singularity containers are currently not supported on the OSGConnect access point. The guide assumes that you have your own Linux machine where you can install Singularity and execute commands via sudo. This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on OSG, please see Use Containers on the OSG","title":"Create a Singularity Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#install-singularity","text":"Install Singularity to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. sudo is required to build the images.","title":"Install Singularity"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name.","title":"Identify Components"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#editing-the-build-spec","text":"Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install See the Singularity documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install Once your build spec is ready, you can \"build\" the container image by running this command: $ sudo singularity build my-container.sif image.def Note that sudo here is currently required. Once the image is built, you can upload it to Stash, test it on OSGConenct, and use it in your HTCondor jobs. This is all described in the container guide .","title":"Editing the Build Spec"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/","text":"Use Containers on the OSG \u00b6 Docker and Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on OSG, it does not matter whether you provide a Docker or Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Singularity images over Docker images include if an image already exists, external to OSG distribution preferences, and if you have experience building images in one for format and not the other. Because OSG is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container. Exploring Images on the Access Points \u00b6 Just like it is important to test your codes and jobs at a small scale, you should make sure that your container is working correctly. One way to explore how OSG sees your container images, is to explore them on the OSG Connect access points. Start an interactive session with the Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest/ This will give you an interactive shell in an Ubuntu 20.04 container, with your current working directory mounted under /srv. You can explore the container and test your code with for example your own inputs from your home directory. Once you are down exploring, exit the container by running exit or with CTRL+D OSG-Provided Images \u00b6 The OSG Team maintains a set of images that are already in the OSG Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the OSG Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. For example, this is what a submit file might look like to run your job within our EL8 container: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Custom Singularity Images \u00b6 If you already have software in the form of a .sif Singuilarity file, and that file is within the supported data sizes , you can stage the .sif file with your job. The image will be resused for each job, and thus the preferred transfer method is Stash . Store the .sif file under /public/$USERNAME/ , and then use the stash url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSG Connect username. Example: +SingularityImage = \"stash:///osgconnect/public/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that Stash aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Singularity images can be found in the Singularity Images Guide . Custom Docker Images \u00b6 If you would prefer to create or use an existing Docker Hub container, for example an authoritative container for your software which already exists in Docker Hub, OSG can distribute the image for you via CVMFS. The result is a synchronized copy of the image under /cvmfs/singularity.opensciencegrid.org/ which is cached and available to the execution nodes. Creating and/or registering a Docker image is described in the Docker Images Guide . To run a job with a Docker image, use the +SingularityImage to specify the image the job should be using. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Another example would be if your Docker Hub username is alice and you created a container called ncbi-blast added to the OSG Singularity repository, your submit file will include: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/alice/ncbi-blast\" <other usual submit file lines> queue Using Containers from Non-OSG Connect Access Points \u00b6 Users on non-OSG Connect access points can use all the container functionality described above, but will have to use slightly more complex job submit files. This is because the OSG Connect access points uses job transforms to update the jobs based on the +SingularityImage attribute, and OSG Connect users also have direct access to Stash. To run a Singularity image from a non-OSG Connect access point, include a job requirements , and specify a method for image transfer. For example: Requirements = HAS_SINGULARITY == TRUE && SINGULARITY_CAN_USE_SIF = TRUE transfer_input_files = http://datastore.host/mycontainer.sif +SingularityImage = \"./mycontainer.sif\" <other usual submit file lines> queue For images available on CVMFS, just add job requirements : Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Frequently Asked Questions / Common Issues \u00b6 FATAL: kernel too old \u00b6 If you get a FATAL: kernel too old error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000 Learning More \u00b6 For more information about Docker, please see: Docker Home Page and Singularity, please see: Singularity Home Page Singularity has become the preferred containerization method in scientific computing. The following talk describes Singularity for scientific computing:","title":"Use Containers on the OSG "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#use-containers-on-the-osg","text":"Docker and Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on OSG, it does not matter whether you provide a Docker or Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Singularity images over Docker images include if an image already exists, external to OSG distribution preferences, and if you have experience building images in one for format and not the other. Because OSG is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container.","title":"Use Containers on the OSG"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#exploring-images-on-the-access-points","text":"Just like it is important to test your codes and jobs at a small scale, you should make sure that your container is working correctly. One way to explore how OSG sees your container images, is to explore them on the OSG Connect access points. Start an interactive session with the Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest/ This will give you an interactive shell in an Ubuntu 20.04 container, with your current working directory mounted under /srv. You can explore the container and test your code with for example your own inputs from your home directory. Once you are down exploring, exit the container by running exit or with CTRL+D","title":"Exploring Images on the Access Points"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#osg-provided-images","text":"The OSG Team maintains a set of images that are already in the OSG Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the OSG Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. For example, this is what a submit file might look like to run your job within our EL8 container: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue","title":"OSG-Provided Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#custom-singularity-images","text":"If you already have software in the form of a .sif Singuilarity file, and that file is within the supported data sizes , you can stage the .sif file with your job. The image will be resused for each job, and thus the preferred transfer method is Stash . Store the .sif file under /public/$USERNAME/ , and then use the stash url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSG Connect username. Example: +SingularityImage = \"stash:///osgconnect/public/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that Stash aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Singularity images can be found in the Singularity Images Guide .","title":"Custom Singularity Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#custom-docker-images","text":"If you would prefer to create or use an existing Docker Hub container, for example an authoritative container for your software which already exists in Docker Hub, OSG can distribute the image for you via CVMFS. The result is a synchronized copy of the image under /cvmfs/singularity.opensciencegrid.org/ which is cached and available to the execution nodes. Creating and/or registering a Docker image is described in the Docker Images Guide . To run a job with a Docker image, use the +SingularityImage to specify the image the job should be using. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Another example would be if your Docker Hub username is alice and you created a container called ncbi-blast added to the OSG Singularity repository, your submit file will include: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/alice/ncbi-blast\" <other usual submit file lines> queue","title":"Custom Docker Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#using-containers-from-non-osg-connect-access-points","text":"Users on non-OSG Connect access points can use all the container functionality described above, but will have to use slightly more complex job submit files. This is because the OSG Connect access points uses job transforms to update the jobs based on the +SingularityImage attribute, and OSG Connect users also have direct access to Stash. To run a Singularity image from a non-OSG Connect access point, include a job requirements , and specify a method for image transfer. For example: Requirements = HAS_SINGULARITY == TRUE && SINGULARITY_CAN_USE_SIF = TRUE transfer_input_files = http://datastore.host/mycontainer.sif +SingularityImage = \"./mycontainer.sif\" <other usual submit file lines> queue For images available on CVMFS, just add job requirements : Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue","title":"Using Containers from Non-OSG Connect Access Points"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#frequently-asked-questions-common-issues","text":"","title":"Frequently Asked Questions / Common Issues"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#fatal-kernel-too-old","text":"If you get a FATAL: kernel too old error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000","title":"FATAL: kernel too old"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#learning-more","text":"For more information about Docker, please see: Docker Home Page and Singularity, please see: Singularity Home Page Singularity has become the preferred containerization method in scientific computing. The following talk describes Singularity for scientific computing:","title":"Learning More"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/","text":"Example Software Compilation \u00b6 Example of Compilng Software For Use In OSG Connect \u00b6 Introduction \u00b6 Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides a detailed example of compiling software for use in OSG Connect. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. However, the steps described in this example are applicable to many other software. For a general introduction to software compilation in OSG Connect, please see Compiling Software for OSG Connect . Specifically, this guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ . Compile Samtools Without CRAM Support \u00b6 Step 1. Acquire Samtools source code \u00b6 Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Connect login node and use wget to download the source code directly and extract the tarball: [user@login ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@login ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory. Step 2. Read through installation instructions \u00b6 What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported in OSG Connect. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Connect login node, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@login ~]$ ls /usr/lib* | grep libcurl [user@login ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation. Step 3. Perform Samtools compilation \u00b6 We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 4. Make our software portable \u00b6 Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ mv my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 5. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $PATH # run samtools commands ... Compile Samtools With CRAM Support \u00b6 This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support. Step 2. Read through installation instructions, continued \u00b6 From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@login ~]$ ls /usr/lib* | grep libz [user@login ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps. Luckily for us, bzip2 version 1.0.6 is available as a module and this module includes access to a .a library file. We will use this module for our Samtools compilation. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . liblzma however is not currently available as a module and our next step will be to install liblzma . Step 3. Compile liblzma \u00b6 To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@login ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@login ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@login ~]$ cd xz-5.2.5/ [user@login xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@login xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ make [user@login xz-5.2.5]$ make install [user@login xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@login xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@login xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@login xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH Step 4. Load bzip2 module \u00b6 After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@login xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . Step 5. Compile Samtools \u00b6 After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 6. Make our software portable \u00b6 Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ cd my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 7. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= TRUE) request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes two important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system and which has access to OSG Connect software modules. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH = $_CONDOR_SCRATCH_DIR /xz-5.2.5/lib: $LD_LIBRARY_PATH export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $_CONDOR_SCRATCH_DIR /xz-5.2.5/bin: $PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Example Software Compilation "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#example-software-compilation","text":"","title":"Example Software Compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#example-of-compilng-software-for-use-in-osg-connect","text":"","title":"Example of Compilng Software For Use In OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#introduction","text":"Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides a detailed example of compiling software for use in OSG Connect. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. However, the steps described in this example are applicable to many other software. For a general introduction to software compilation in OSG Connect, please see Compiling Software for OSG Connect . Specifically, this guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ .","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#compile-samtools-without-cram-support","text":"","title":"Compile Samtools Without CRAM Support"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-1-acquire-samtools-source-code","text":"Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Connect login node and use wget to download the source code directly and extract the tarball: [user@login ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@login ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory.","title":"Step 1. Acquire Samtools source code"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-2-read-through-installation-instructions","text":"What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported in OSG Connect. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Connect login node, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@login ~]$ ls /usr/lib* | grep libcurl [user@login ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation.","title":"Step 2. Read through installation instructions"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-3-perform-samtools-compilation","text":"We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 3. Perform Samtools compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-4-make-our-software-portable","text":"Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ mv my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 4. Make our software portable"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-5-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $PATH # run samtools commands ...","title":"Step 5. Use Samtools in our jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#compile-samtools-with-cram-support","text":"This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support.","title":"Compile Samtools With CRAM Support"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-2-read-through-installation-instructions-continued","text":"From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@login ~]$ ls /usr/lib* | grep libz [user@login ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps. Luckily for us, bzip2 version 1.0.6 is available as a module and this module includes access to a .a library file. We will use this module for our Samtools compilation. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . liblzma however is not currently available as a module and our next step will be to install liblzma .","title":"Step 2. Read through installation instructions, continued"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-3-compile-liblzma","text":"To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@login ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@login ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@login ~]$ cd xz-5.2.5/ [user@login xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@login xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ make [user@login xz-5.2.5]$ make install [user@login xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@login xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@login xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@login xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH","title":"Step 3. Compile liblzma"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-4-load-bzip2-module","text":"After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@login xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files. To learn more about using modules, please see Accessing Software using Distributed Environment Modules .","title":"Step 4. Load bzip2 module"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-5-compile-samtools","text":"After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 5. Compile Samtools"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-6-make-our-software-portable","text":"Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ cd my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 6. Make our software portable"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-7-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= TRUE) request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes two important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system and which has access to OSG Connect software modules. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH = $_CONDOR_SCRATCH_DIR /xz-5.2.5/lib: $LD_LIBRARY_PATH export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $_CONDOR_SCRATCH_DIR /xz-5.2.5/bin: $PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Step 7. Use Samtools in our jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/","text":"Access Software using Distributed Environment Modules \u00b6 Introduction \u00b6 This page covers the use of distributed environment modules on RHEL7 compute nodes in the OSG computing environment. Environment modules provide users with an easy way to access different versions of software, libraries, and compilers. Currently, OSG Connect login nodes and a majority of OSG compute nodes use the RHEL7 operating system. However, given the distributed nature of the OSG, there are at times a number of RHEL6 and RHEL8 compute nodes also available to users. For users that require, or who can also use, RHEL6 compute nodes for their applications, please contact us to learn more about modules specifically available to RHEL6 compute nodes. List Available Modules on OSG Connect \u00b6 First sign in to your OSG Connect login node. Then use module avail to see all available software applications and libraries: $ module avail ------------ /cvmfs/connect.opensciencegrid.org/modules/modulefiles/linux-rhel7-x86_64/Core ------------ autoconf/2.69 libiconv/1.15 py-idna/2.5-py3.7 (D) automake/1.16.1 libjpeg-turbo/1.5.3 py-ipaddress/1.0.18-py2.7 binutils/2.31.1-py2.7 libjpeg-turbo/1.5.90 (D) py-kiwisolver/1.0.1-py2.7 binutils/2.31.1-py3.7 (D) libpciaccess/0.13.5 py-kiwisolver/1.0.1-py3.7 (D) bison/3.0.5-py2.7 libpng/1.6.34 py-lit/0.5.0-py2.7 bison/3.0.5 (D) libpthread-stubs/0.4 py-mako/1.0.4-py2.7 boost/1.68.0-py2.7 libsigsegv/2.11 py-markupsafe/1.0-py2.7 boost/1.68.0-py3.7 (D) libsm/1.2.2 py-matplotlib/2.2.3-py2.7 bowtie2/2.3.4.1-py2.7 libtiff/4.0.9 py-matplotlib/3.0.0-py3.7 (D) bullet3/2.87 libtool/2.4.6 py-nose/1.3.7-py2.7 bwa/0.7.17 libx11/1.6.5 py-numexpr/2.6.5-py2.7 bzip2/1.0.6 libxau/1.0.8 py-numexpr/2.6.5-py3.7 (D) cairo/1.14.12-py2.7 libxcb/1.13 py-numpy/1.15.2-py2.7 cctools/7.0.8-py2.7 libxdamage/1.1.4 py-numpy/1.15.2-py3.7 (D) cfitsio/3.450 libxdmcp/1.1.2 py-pandas/0.23.4-py2.7 charmpp/6.8.2 libxext/1.3.3 py-pandas/0.23.4-py3.7 (D) ... more modules ... Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Please know, there are more than 200 modules available to OSG Connect users. The above is not a complete list of all available modules as the list is quite long and subject to change as new sofware and libraries get added. How To Use Environment Modules \u00b6 In order to use the software or libraries provided in a module, you will need to 'load' that particular module. To load a particular module, use module load [modulename] . For example: $ module load python/2.7.14-k To see currently loaded modules, use module list . For example: $ module list Currently Loaded Modules: 1) python/2.7.14-k To unload a package and remove a module from your environment, use module unload [modulename] . For example: $ module unload python/2.7.14-k Finally, module help will give you more detailed information. Use Environment Modules in Jobs \u00b6 Add module commands to executable script \u00b6 To use environment modules within a job, use the same module load command described above inside your job's main \"executable\" to load your software and then run it. For example: 1 2 3 4 #!/bin/bash module load python/2.7.14-k python myscript.py Set appropriate submit file requirements \u00b6 Not all resources available through OSG Connect support distributed environment modules. In order to ensure that your jobs will have access to OSG Connect modules, you will need to add the following to your HTCondor submit file: Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\") or append the above requirements if you already have other requirements specified: Requirements = [Other requirements ] && (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")","title":"Access Software using Distributed Environment Modules "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#access-software-using-distributed-environment-modules","text":"","title":"Access Software using Distributed Environment Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#introduction","text":"This page covers the use of distributed environment modules on RHEL7 compute nodes in the OSG computing environment. Environment modules provide users with an easy way to access different versions of software, libraries, and compilers. Currently, OSG Connect login nodes and a majority of OSG compute nodes use the RHEL7 operating system. However, given the distributed nature of the OSG, there are at times a number of RHEL6 and RHEL8 compute nodes also available to users. For users that require, or who can also use, RHEL6 compute nodes for their applications, please contact us to learn more about modules specifically available to RHEL6 compute nodes.","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#list-available-modules-on-osg-connect","text":"First sign in to your OSG Connect login node. Then use module avail to see all available software applications and libraries: $ module avail ------------ /cvmfs/connect.opensciencegrid.org/modules/modulefiles/linux-rhel7-x86_64/Core ------------ autoconf/2.69 libiconv/1.15 py-idna/2.5-py3.7 (D) automake/1.16.1 libjpeg-turbo/1.5.3 py-ipaddress/1.0.18-py2.7 binutils/2.31.1-py2.7 libjpeg-turbo/1.5.90 (D) py-kiwisolver/1.0.1-py2.7 binutils/2.31.1-py3.7 (D) libpciaccess/0.13.5 py-kiwisolver/1.0.1-py3.7 (D) bison/3.0.5-py2.7 libpng/1.6.34 py-lit/0.5.0-py2.7 bison/3.0.5 (D) libpthread-stubs/0.4 py-mako/1.0.4-py2.7 boost/1.68.0-py2.7 libsigsegv/2.11 py-markupsafe/1.0-py2.7 boost/1.68.0-py3.7 (D) libsm/1.2.2 py-matplotlib/2.2.3-py2.7 bowtie2/2.3.4.1-py2.7 libtiff/4.0.9 py-matplotlib/3.0.0-py3.7 (D) bullet3/2.87 libtool/2.4.6 py-nose/1.3.7-py2.7 bwa/0.7.17 libx11/1.6.5 py-numexpr/2.6.5-py2.7 bzip2/1.0.6 libxau/1.0.8 py-numexpr/2.6.5-py3.7 (D) cairo/1.14.12-py2.7 libxcb/1.13 py-numpy/1.15.2-py2.7 cctools/7.0.8-py2.7 libxdamage/1.1.4 py-numpy/1.15.2-py3.7 (D) cfitsio/3.450 libxdmcp/1.1.2 py-pandas/0.23.4-py2.7 charmpp/6.8.2 libxext/1.3.3 py-pandas/0.23.4-py3.7 (D) ... more modules ... Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Please know, there are more than 200 modules available to OSG Connect users. The above is not a complete list of all available modules as the list is quite long and subject to change as new sofware and libraries get added.","title":"List Available Modules on OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#how-to-use-environment-modules","text":"In order to use the software or libraries provided in a module, you will need to 'load' that particular module. To load a particular module, use module load [modulename] . For example: $ module load python/2.7.14-k To see currently loaded modules, use module list . For example: $ module list Currently Loaded Modules: 1) python/2.7.14-k To unload a package and remove a module from your environment, use module unload [modulename] . For example: $ module unload python/2.7.14-k Finally, module help will give you more detailed information.","title":"How To Use Environment Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#use-environment-modules-in-jobs","text":"","title":"Use Environment Modules in Jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#add-module-commands-to-executable-script","text":"To use environment modules within a job, use the same module load command described above inside your job's main \"executable\" to load your software and then run it. For example: 1 2 3 4 #!/bin/bash module load python/2.7.14-k python myscript.py","title":"Add module commands to executable script"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#set-appropriate-submit-file-requirements","text":"Not all resources available through OSG Connect support distributed environment modules. In order to ensure that your jobs will have access to OSG Connect modules, you will need to add the following to your HTCondor submit file: Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\") or append the above requirements if you already have other requirements specified: Requirements = [Other requirements ] && (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")","title":"Set appropriate submit file requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/","text":"Control Where Your Jobs Run / Job Requirements \u00b6 By default, your jobs will match any available spot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites. Requirements \u00b6 The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 8 nodes. requirements = OSGVO_OS_STRING == \"RHEL 8\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 7 or 8, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 7\" || OSGVO_OS_STRING == \"RHEL 8\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True Additional Feature-Specific Attributes \u00b6 There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the OSGVO_OS_STRING attribute run: $ condor_status -af OSGVO_OS_STRING | sort -u RHEL 7 RHEL 8 This means that we can specify an OS version of RHEL 7 or RHEL 8 . Alternatively you will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. HAS_SINGULARITY - Boolean specifying the need to use Singularity containers in your job. HAS_MODULES - Boolean specifying the need to use modules in your job. module load ... or not. OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Common values are RHEL 7 and RHEL 8 . Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. CUDACapability - For GPU jobs, specifies the CUDA compute capability. See our GPU guide for more details. Specifying Sites / Avoiding Sites \u00b6 To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Control Where Your Jobs Run / Job Requirements "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#control-where-your-jobs-run-job-requirements","text":"By default, your jobs will match any available spot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites.","title":"Control Where Your Jobs Run / Job Requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#requirements","text":"The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 8 nodes. requirements = OSGVO_OS_STRING == \"RHEL 8\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 7 or 8, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 7\" || OSGVO_OS_STRING == \"RHEL 8\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True","title":"Requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#additional-feature-specific-attributes","text":"There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the OSGVO_OS_STRING attribute run: $ condor_status -af OSGVO_OS_STRING | sort -u RHEL 7 RHEL 8 This means that we can specify an OS version of RHEL 7 or RHEL 8 . Alternatively you will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. HAS_SINGULARITY - Boolean specifying the need to use Singularity containers in your job. HAS_MODULES - Boolean specifying the need to use modules in your job. module load ... or not. OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Common values are RHEL 7 and RHEL 8 . Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. CUDACapability - For GPU jobs, specifies the CUDA compute capability. See our GPU guide for more details.","title":"Additional Feature-Specific Attributes"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#specifying-sites-avoiding-sites","text":"To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Specifying Sites / Avoiding Sites"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/","text":"Using Software on the Open Science Pool \u00b6 Overview of Software Options \u00b6 There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). In general, the OSPool can support most popular, open source software that fit the distributed high throughput computing model. At present, we do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers, and preinstalled software via modules. Install Your Own Software \u00b6 For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. Installing your own software can be performed interactively on your assigned login server. More information about how to install your own software from source code can be found at Compiling Software for the OSPool . When installing software on an OSG Connect login node, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 7 OS used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 7 OS, which can be specified by the requirements attribute of your HTCondor submit files - an example is provided in the software compilation guide linked above. Be sure to also review our Introduction to Data Management on OSG Connect guide to determine the appropriate method for transferring software with your jobs. Use Precompiled Binaries and Prebuilt Executables \u00b6 Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1.) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2.) some softwares have system and hardware dependencies that must be met in order to run properly, and 3.) the available binaries may not have been compiled with the feaures or configuration needed for your work. Use Docker and Singularity Containers \u00b6 Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Singularity and Docker containers - the latter will be converted to a Singularity image and added to the OSG container image repository. Users can choose from a set of pre-defined containers already available within OSG, or can use published or custom made containers. More details on how to use containers on the OSPool can be found in our Docker and Singularity Containers guide. Access Software In Distributed Modules \u00b6 Currently, the OSPool provides over 200 preinstalled software, libraries, and compiliers via distributed environment modules. Modules provide a streamlined option for working with different software versions as well as managing library dependencies for software. To run jobs that use modules, your HTCondor submit files must include additional requirements to direct your jobs to appropriate compute nodes within OSG. More details about using modules can be found here . Ask for Helpk \u00b6 If you are not sure which of the above options might be best for your software, we're happy to help! Just contact us at support@opensciencegrid.org . Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Using Software on the Open Science Pool "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#using-software-on-the-open-science-pool","text":"","title":"Using Software on the Open Science Pool"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#overview-of-software-options","text":"There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). In general, the OSPool can support most popular, open source software that fit the distributed high throughput computing model. At present, we do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers, and preinstalled software via modules.","title":"Overview of Software Options"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#install-your-own-software","text":"For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. Installing your own software can be performed interactively on your assigned login server. More information about how to install your own software from source code can be found at Compiling Software for the OSPool . When installing software on an OSG Connect login node, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 7 OS used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 7 OS, which can be specified by the requirements attribute of your HTCondor submit files - an example is provided in the software compilation guide linked above. Be sure to also review our Introduction to Data Management on OSG Connect guide to determine the appropriate method for transferring software with your jobs.","title":"Install Your Own Software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#use-precompiled-binaries-and-prebuilt-executables","text":"Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1.) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2.) some softwares have system and hardware dependencies that must be met in order to run properly, and 3.) the available binaries may not have been compiled with the feaures or configuration needed for your work.","title":"Use Precompiled Binaries and Prebuilt Executables"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#use-docker-and-singularity-containers","text":"Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Singularity and Docker containers - the latter will be converted to a Singularity image and added to the OSG container image repository. Users can choose from a set of pre-defined containers already available within OSG, or can use published or custom made containers. More details on how to use containers on the OSPool can be found in our Docker and Singularity Containers guide.","title":"Use Docker and Singularity Containers"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#access-software-in-distributed-modules","text":"Currently, the OSPool provides over 200 preinstalled software, libraries, and compiliers via distributed environment modules. Modules provide a streamlined option for working with different software versions as well as managing library dependencies for software. To run jobs that use modules, your HTCondor submit files must include additional requirements to direct your jobs to appropriate compute nodes within OSG. More details about using modules can be found here .","title":"Access Software In Distributed Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#ask-for-helpk","text":"If you are not sure which of the above options might be best for your software, we're happy to help! Just contact us at support@opensciencegrid.org . Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Ask for Helpk"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-request/","text":"Request Help with Your Software \u00b6 A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@opensciencegrid.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Request Help with Your Software "},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-request/#request-help-with-your-software","text":"A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@opensciencegrid.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Request Help with Your Software"},{"location":"overview/references/acknowledgeOSG/","text":"Acknowledge the OSG \u00b6 This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/acknowledgeOSG/#acknowledge-the-osg","text":"This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/contact-information/","text":"Contact OSG for non-Support Inquiries \u00b6 For media contact, leadership, or general questions about OSG, please see our main website To request a user account on OSG Connect, please visit the website . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@opensciencegrid.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/contact-information/#contact-osg-for-non-support-inquiries","text":"For media contact, leadership, or general questions about OSG, please see our main website To request a user account on OSG Connect, please visit the website . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@opensciencegrid.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/frequently-asked-questions-faq-/","text":"Frequently Asked Questions \u00b6 Getting Started \u00b6 Who is eligible to become the user of OSG Connect? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to become an OSG Connect user. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. Researchers outside of the U.S. are asked to first contact us directly to discuss membership. How do I become an user of OSG Connect? Please follow the steps outlined in the Sign Up process . Software \u00b6 What software packages are available? In general, we support most software that fit the distributed high throughput computing model. Users are encouraged to download and install their own software. For some software, we support distributed software modules listed here . Software can be added to the modules upon request. Additionally, users may install their software into a Docker container which can run on OSG as a Singularity image. See this guide for more information. How do I access a specific software application? We have implemented modules within OSG Connect to manage the software that is available to users. Modules allow for easy access to a number of software and version options. Our Accessing Software using Distributed Environment Modules page provides more details on how to use modules in OSG Connect. Are there any restrictions on installing commercial softwares? We can only directly support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's /home directory with HTCondor transferring to jobs, but should not be staged in OSG's public data staging locations (see https://support.opensciencegrid.org/support/solutions/articles/12000002985-data-management-and-policies). Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? Yes. Please contact support@opensciencegrid.org . Running Jobs \u00b6 What type of computation is a good match or NOT a good match for OSG Connect? OSG Connect is a high throughput computing system. You can get the most of out OSG Connect resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our \"Is OSG for You?\" page for more details on how to determine if your work matches up well with OSG Connect's model. What job scheduler is being used on OSG Connect? We use use the task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our QuickStart guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user should not exceed 10,000. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally). Data Storage and Transfer \u00b6 What is the best way to process large volume of data? Use the Stash data system to stage large volumes of data. Please refer the section Data Solutions for more details. How do I transfer my data to and from OSG Connect? You can transfer data using scp or rsync. See Using scp To Transfer Files To OSG Connect for more details. How public is /public? The data under your /public location is discoverable and readable by anyone in the world. Data in /public is made public over http/https (via https://stash.osgconnect.net/public/ ) and mirrored to /cvmfs/stash.osgstorage.org/osgconnect/public/ (for use with stashcp ) which is mounted on a large number of systems around the world. Is there any support for private data? OSG currently has no storage appropriate for HIPAA data. If you do not want your data to be downloadable by anyone, and it\u2019s small enough for HTCondor file transfer (i.e. <100MB per file and <500MB total per job), then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files , in the submit file). If your data must remain private and is too large for HTCondor file transfer, then it\u2019s not a good fit for the \u201copen\u201d environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems. Lastly, our data storage locations are not backed up nor are they intended for long-term storage. Can I get a quota increase? Contact us if you think you'll need a quota increase for /home or /public to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The only place you can currently see your quota status is in the login messages. Workflow Management \u00b6 How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files to, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support and distribute DAGMan, Pegasus, and Swift for workflow management. Workshops and Training \u00b6 Do you plan to offer training sessions and workshop? We plan to offer workshops for the researchers on multiple locations, including an annual, week-long summer school for OSG users. Please check our events page for further information about workshop dates and locations. Who may attend OSG workshops? Workshops are typically open to students, post docs, staff and faculty. What are the topics covered in a typical workshop? We typically cover shell scripting, python (or R) programming, version control with git and distributed high throughout computing. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we would be grateful to have you acknowledge OSG in your presentations and publications. For example, you can add the following in your acknowledgements section: \"This research was done using resources provided by the OSG, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science.\" We recommend the following references for citations 1) Pordes, R. et al. (2007). \"The Open Science Grid\", J. Phys. Conf. Ser. 78, 012057.doi:10.1088/1742-6596/78/1/012057. 2) Sfiligoi, I., Bradley, D. C., Holzman, B., Mhashilkar, P., Padhi, S. and Wurthwein, F. (2009). \"The Pilot Way to Grid Resources Using glideinWMS\", 2009 WRI World Congress on Computer Science and Information Engineering, Vol. 2, pp. 428\u2013432. doi:10.1109/CSIE.2009.950.","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions-faq-/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions-faq-/#getting-started","text":"Who is eligible to become the user of OSG Connect? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to become an OSG Connect user. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. Researchers outside of the U.S. are asked to first contact us directly to discuss membership. How do I become an user of OSG Connect? Please follow the steps outlined in the Sign Up process .","title":"Getting Started"},{"location":"overview/references/frequently-asked-questions-faq-/#software","text":"What software packages are available? In general, we support most software that fit the distributed high throughput computing model. Users are encouraged to download and install their own software. For some software, we support distributed software modules listed here . Software can be added to the modules upon request. Additionally, users may install their software into a Docker container which can run on OSG as a Singularity image. See this guide for more information. How do I access a specific software application? We have implemented modules within OSG Connect to manage the software that is available to users. Modules allow for easy access to a number of software and version options. Our Accessing Software using Distributed Environment Modules page provides more details on how to use modules in OSG Connect. Are there any restrictions on installing commercial softwares? We can only directly support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's /home directory with HTCondor transferring to jobs, but should not be staged in OSG's public data staging locations (see https://support.opensciencegrid.org/support/solutions/articles/12000002985-data-management-and-policies). Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? Yes. Please contact support@opensciencegrid.org .","title":"Software"},{"location":"overview/references/frequently-asked-questions-faq-/#running-jobs","text":"What type of computation is a good match or NOT a good match for OSG Connect? OSG Connect is a high throughput computing system. You can get the most of out OSG Connect resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our \"Is OSG for You?\" page for more details on how to determine if your work matches up well with OSG Connect's model. What job scheduler is being used on OSG Connect? We use use the task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our QuickStart guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user should not exceed 10,000. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally).","title":"Running Jobs"},{"location":"overview/references/frequently-asked-questions-faq-/#data-storage-and-transfer","text":"What is the best way to process large volume of data? Use the Stash data system to stage large volumes of data. Please refer the section Data Solutions for more details. How do I transfer my data to and from OSG Connect? You can transfer data using scp or rsync. See Using scp To Transfer Files To OSG Connect for more details. How public is /public? The data under your /public location is discoverable and readable by anyone in the world. Data in /public is made public over http/https (via https://stash.osgconnect.net/public/ ) and mirrored to /cvmfs/stash.osgstorage.org/osgconnect/public/ (for use with stashcp ) which is mounted on a large number of systems around the world. Is there any support for private data? OSG currently has no storage appropriate for HIPAA data. If you do not want your data to be downloadable by anyone, and it\u2019s small enough for HTCondor file transfer (i.e. <100MB per file and <500MB total per job), then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files , in the submit file). If your data must remain private and is too large for HTCondor file transfer, then it\u2019s not a good fit for the \u201copen\u201d environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems. Lastly, our data storage locations are not backed up nor are they intended for long-term storage. Can I get a quota increase? Contact us if you think you'll need a quota increase for /home or /public to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The only place you can currently see your quota status is in the login messages.","title":"Data Storage and Transfer"},{"location":"overview/references/frequently-asked-questions-faq-/#workflow-management","text":"How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files to, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support and distribute DAGMan, Pegasus, and Swift for workflow management.","title":"Workflow Management"},{"location":"overview/references/frequently-asked-questions-faq-/#workshops-and-training","text":"Do you plan to offer training sessions and workshop? We plan to offer workshops for the researchers on multiple locations, including an annual, week-long summer school for OSG users. Please check our events page for further information about workshop dates and locations. Who may attend OSG workshops? Workshops are typically open to students, post docs, staff and faculty. What are the topics covered in a typical workshop? We typically cover shell scripting, python (or R) programming, version control with git and distributed high throughout computing. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we would be grateful to have you acknowledge OSG in your presentations and publications. For example, you can add the following in your acknowledgements section: \"This research was done using resources provided by the OSG, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science.\" We recommend the following references for citations 1) Pordes, R. et al. (2007). \"The Open Science Grid\", J. Phys. Conf. Ser. 78, 012057.doi:10.1088/1742-6596/78/1/012057. 2) Sfiligoi, I., Bradley, D. C., Holzman, B., Mhashilkar, P., Padhi, S. and Wurthwein, F. (2009). \"The Pilot Way to Grid Resources Using glideinWMS\", 2009 WRI World Congress on Computer Science and Information Engineering, Vol. 2, pp. 428\u2013432. doi:10.1109/CSIE.2009.950.","title":"Workshops and Training"},{"location":"overview/references/gracc/","text":"OSG Accounting (GRACC) \u00b6 GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ To drill down on your own project and/or user, go to Payload Jobs . Under the Project or User drop-downs, find your project/user. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Group By time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Group By .","title":"OSG Accounting (GRACC)"},{"location":"overview/references/gracc/#osg-accounting-gracc","text":"GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ To drill down on your own project and/or user, go to Payload Jobs . Under the Project or User drop-downs, find your project/user. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Group By time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Group By .","title":"OSG Accounting (GRACC)"},{"location":"overview/references/policy/","text":"Policies for Using OSG Connect and the OSPool \u00b6 Access to OSG Connect and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all OSG Connect users. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG Connect service, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG Connect is only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the access points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the login nodes, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the login nodes. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on the OSG Connect login nodes. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG Connect filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG Connect policies for data storage and per-job transfers . Some OSG Connect storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG Connect filesystems or computed in OSG. Similarly, users should follow all licensing requirements when storing and executing software via OSG Connect submit servers. (See \u201cSoftware in OSG Connect\u201d.) Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@opensciencegrid.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Connect and the OSPool "},{"location":"overview/references/policy/#policies-for-using-osg-connect-and-the-ospool","text":"Access to OSG Connect and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all OSG Connect users. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG Connect service, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG Connect is only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the access points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the login nodes, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the login nodes. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on the OSG Connect login nodes. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG Connect filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG Connect policies for data storage and per-job transfers . Some OSG Connect storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG Connect filesystems or computed in OSG. Similarly, users should follow all licensing requirements when storing and executing software via OSG Connect submit servers. (See \u201cSoftware in OSG Connect\u201d.) Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@opensciencegrid.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Connect and the OSPool"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/","text":"Generate SSH Keys and Activate Your OSG Login \u00b6 Overview \u00b6 OSG Connect requires SSH-key-based logins. You need to follow a two-step process to set up the SSH key to your account. Generate a SSH key pair. Add your public key to the submit host by uploading it to your OSG Connect user profile (via the OSG Connect website). After completing the process, you can log in from a local computer (your laptop or desktop) to the OSG Connect login node assigned using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the login node. Step 1: Generate SSH Keys \u00b6 We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to OSG Connect, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop). Unix-based operating system (Linux/Mac) or latest Windows 10 versions \u00b6 We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub) Windows, using Putty to log in \u00b6 If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy. Step 2: Add the public SSH key to login node \u00b6 To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours. Can I Use Multiple Keys? \u00b6 Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile. Logging In \u00b6 After following the steps above to upload your key and it's been a few hours, you should be able to log in to OSG Connect. Determine which login node to use \u00b6 Before you can connect, you will need to know which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to www.osgconnect.net and sign in with your CILogin. Click \"Profile\" in the top right corner. The assigned login nodes are listed in the left side box. Make note of the address of your assigned login node as you will use this to connect to OSG Connect. For Mac, Linux, or newer versions of Windows \u00b6 Open a terminal and type in: ssh <your_osg_connect_username>@<your_osg_login_node> It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in. For older versions of Windows \u00b6 On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in step 5 above. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Generate SSH Keys and Activate Your OSG Login "},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#generate-ssh-keys-and-activate-your-osg-login","text":"","title":"Generate SSH Keys and Activate Your OSG Login"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#overview","text":"OSG Connect requires SSH-key-based logins. You need to follow a two-step process to set up the SSH key to your account. Generate a SSH key pair. Add your public key to the submit host by uploading it to your OSG Connect user profile (via the OSG Connect website). After completing the process, you can log in from a local computer (your laptop or desktop) to the OSG Connect login node assigned using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the login node.","title":"Overview"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#step-1-generate-ssh-keys","text":"We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to OSG Connect, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop).","title":"Step 1: Generate SSH Keys"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#unix-based-operating-system-linuxmac-or-latest-windows-10-versions","text":"We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub)","title":"Unix-based operating system (Linux/Mac) or latest Windows 10 versions"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#windows-using-putty-to-log-in","text":"If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy.","title":"Windows, using Putty to log in"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#step-2-add-the-public-ssh-key-to-login-node","text":"To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours.","title":"Step 2: Add the public SSH key to login node"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#can-i-use-multiple-keys","text":"Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile.","title":"Can I Use Multiple Keys?"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#logging-in","text":"After following the steps above to upload your key and it's been a few hours, you should be able to log in to OSG Connect.","title":"Logging In"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#determine-which-login-node-to-use","text":"Before you can connect, you will need to know which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to www.osgconnect.net and sign in with your CILogin. Click \"Profile\" in the top right corner. The assigned login nodes are listed in the left side box. Make note of the address of your assigned login node as you will use this to connect to OSG Connect.","title":"Determine which login node to use"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#for-mac-linux-or-newer-versions-of-windows","text":"Open a terminal and type in: ssh <your_osg_connect_username>@<your_osg_login_node> It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in.","title":"For Mac, Linux, or newer versions of Windows"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#for-older-versions-of-windows","text":"On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in step 5 above. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so.","title":"For older versions of Windows"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"overview/welcome_and_account_setup/is-it-for-you/","text":"Computation on the Open Science Pool \u00b6 The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated research projects and groups via the OSG Connect service, which the documentation on this site is specific to. Learn more about the services provided by the OSG that can support your HTC workload: For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect Resources to Quickly Learn More \u00b6 Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Live training sessions are advertised/open to those with active accounts on OSG Connect. Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Computation on the Open Science Pool "},{"location":"overview/welcome_and_account_setup/is-it-for-you/#computation-on-the-open-science-pool","text":"The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated research projects and groups via the OSG Connect service, which the documentation on this site is specific to. Learn more about the services provided by the OSG that can support your HTC workload: For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Computation on the Open Science Pool"},{"location":"overview/welcome_and_account_setup/is-it-for-you/#resources-to-quickly-learn-more","text":"Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Live training sessions are advertised/open to those with active accounts on OSG Connect. Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Resources to Quickly Learn More"},{"location":"overview/welcome_and_account_setup/registration-and-login/","text":"Registration and Login for OSG Connect \u00b6 Registration and Login for OSG Connect \u00b6 The major steps to getting started on OSG Connect are: apply for an OSG Connect account meet with an OSG Connect staff member for an short consultation and orientation. join and set your default \"project\" upload .ssh keys to the OSG Connect website Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to login to the OSG Connect submit node. Account Creation \u00b6 Sign in to or create an account \u00b6 Start by creating an OSG Connect account. Visit the OSG Connect web site , then click on the Sign Up button. You will need to agree to our Acceptable Use Policy in order to get to the main log in screen. The main log in screen will prompt you to sign in via your primary institutional affiliation. You'll be directed to a discovery service which asks you what your home institution is. (If you've used CILogon before it may already know your home institution and skip this step.) Locate your institution in the list, or type its name to find matches. No Institutional Identity? If you don't have an institutional identity or can't find your institution on the provided list, either (separately) sign up for a Globus ID and follow the link for that option on this page, or contact the OSG Connect support team for guidance at support@opensciencegrid.org Note that this is the identity that will get linked to your OSG Connect account, so be sure to pick the institution (if you have multiple affiliations) that you would like to associate with your OSG Connect account. After selecting your institution in the discovery service, you'll be taken to your own institution's local sign-in screen. You've probably used it before, and if it looks familiar that's because it's exactly the same web site. Sign in using your campus credentials. When done, you'll return automatically to the OSG Connect portal and can carry on with signup. Returning to OSG Connect ? If you already have an OSG Connect account and are not being taken to your profile page after logging in with your institution's credentials, see our transition guide for how to proceed. After continuing, and allowing certain permissions in the next screen, you'll be asked to create a profile and save changes. If this works successfully, you should see that your membership to OSG is \"pending\" on the right hand side of the screen. Orientation Meeting \u00b6 Once you've applied to join OSG Connect as described above, an OSG Connect support team member will contact you to arrange an initial orientation meeting. This meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSG, and some practical next steps for getting started. Some of these next steps are also listed below. Join a Project \u00b6 As part of the sign up and meeting process, you'll be asked for information related to your research group so that you can be assigned to an accounting project. For more information about this process, see this guide: Start or Join a Project in OSG Connect Generate and Add an SSH key \u00b6 Once your account is created and you're approved, you can generate and upload an SSH key to the OSG Connect website; this key will be duplicated on the OSG Connect login node so that you're able to log in there and submit jobs. To see how to generate and add an SSH key, please visit this page: Step by step instructions to generate and adding an SSH key Log In \u00b6 Once you've gone through the steps above, you should be able to log in to the OSG Connect login node. See the second half of the SSH key guide for details: How to Log Into the OSG Connect Login Node Overview of access procedure and accounting \u00b6 For those interested in details, this section describes some of the background information behind projects. The OSG governs access to grid resources through an accounting framework that assigns each user's jobs to an accounting group or project . As a new user of the OSG, one of the first things to iron out is what project or projects best describe your work. This is more a matter of accountability than of entitlement: it concerns how organizations report to their sponsors and funding agencies on the utilization of resources placed under their administration. To assist in this, OSG Connect uses a group management tool that places users into one or more groups with names such as osg.RDCEP or osg.Extenci . The osg portion of this name differentiates our groups from those of other organizations in the same group management facility. The latter portion identifies the specific project, each with a Principal Investigator or other administrator, that oversees access to resources. Within our web tools, these names are often in mixed case, though you may see them uppercased in some reporting/accounting software. The first step in registration is to create a user account and bind it to other identity information. After that you will enroll in a project group. Once you've enrolled in a group, you'll have the requisite rights to log in to the submit node for the OSG Connect job scheduler, or to transfer data in and out of Stash. Submit node logins are typically via Secure Shell (SSH) using a password or a public key. We'll discuss how to connect further on.","title":"Registration and Login for OSG Connect "},{"location":"overview/welcome_and_account_setup/registration-and-login/#registration-and-login-for-osg-connect","text":"","title":"Registration and Login for OSG Connect"},{"location":"overview/welcome_and_account_setup/registration-and-login/#registration-and-login-for-osg-connect_1","text":"The major steps to getting started on OSG Connect are: apply for an OSG Connect account meet with an OSG Connect staff member for an short consultation and orientation. join and set your default \"project\" upload .ssh keys to the OSG Connect website Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to login to the OSG Connect submit node.","title":"Registration and Login for OSG Connect"},{"location":"overview/welcome_and_account_setup/registration-and-login/#account-creation","text":"","title":"Account Creation"},{"location":"overview/welcome_and_account_setup/registration-and-login/#sign-in-to-or-create-an-account","text":"Start by creating an OSG Connect account. Visit the OSG Connect web site , then click on the Sign Up button. You will need to agree to our Acceptable Use Policy in order to get to the main log in screen. The main log in screen will prompt you to sign in via your primary institutional affiliation. You'll be directed to a discovery service which asks you what your home institution is. (If you've used CILogon before it may already know your home institution and skip this step.) Locate your institution in the list, or type its name to find matches. No Institutional Identity? If you don't have an institutional identity or can't find your institution on the provided list, either (separately) sign up for a Globus ID and follow the link for that option on this page, or contact the OSG Connect support team for guidance at support@opensciencegrid.org Note that this is the identity that will get linked to your OSG Connect account, so be sure to pick the institution (if you have multiple affiliations) that you would like to associate with your OSG Connect account. After selecting your institution in the discovery service, you'll be taken to your own institution's local sign-in screen. You've probably used it before, and if it looks familiar that's because it's exactly the same web site. Sign in using your campus credentials. When done, you'll return automatically to the OSG Connect portal and can carry on with signup. Returning to OSG Connect ? If you already have an OSG Connect account and are not being taken to your profile page after logging in with your institution's credentials, see our transition guide for how to proceed. After continuing, and allowing certain permissions in the next screen, you'll be asked to create a profile and save changes. If this works successfully, you should see that your membership to OSG is \"pending\" on the right hand side of the screen.","title":"Sign in to or create an account"},{"location":"overview/welcome_and_account_setup/registration-and-login/#orientation-meeting","text":"Once you've applied to join OSG Connect as described above, an OSG Connect support team member will contact you to arrange an initial orientation meeting. This meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSG, and some practical next steps for getting started. Some of these next steps are also listed below.","title":"Orientation Meeting"},{"location":"overview/welcome_and_account_setup/registration-and-login/#join-a-project","text":"As part of the sign up and meeting process, you'll be asked for information related to your research group so that you can be assigned to an accounting project. For more information about this process, see this guide: Start or Join a Project in OSG Connect","title":"Join a Project"},{"location":"overview/welcome_and_account_setup/registration-and-login/#generate-and-add-an-ssh-key","text":"Once your account is created and you're approved, you can generate and upload an SSH key to the OSG Connect website; this key will be duplicated on the OSG Connect login node so that you're able to log in there and submit jobs. To see how to generate and add an SSH key, please visit this page: Step by step instructions to generate and adding an SSH key","title":"Generate and Add an SSH key"},{"location":"overview/welcome_and_account_setup/registration-and-login/#log-in","text":"Once you've gone through the steps above, you should be able to log in to the OSG Connect login node. See the second half of the SSH key guide for details: How to Log Into the OSG Connect Login Node","title":"Log In"},{"location":"overview/welcome_and_account_setup/registration-and-login/#overview-of-access-procedure-and-accounting","text":"For those interested in details, this section describes some of the background information behind projects. The OSG governs access to grid resources through an accounting framework that assigns each user's jobs to an accounting group or project . As a new user of the OSG, one of the first things to iron out is what project or projects best describe your work. This is more a matter of accountability than of entitlement: it concerns how organizations report to their sponsors and funding agencies on the utilization of resources placed under their administration. To assist in this, OSG Connect uses a group management tool that places users into one or more groups with names such as osg.RDCEP or osg.Extenci . The osg portion of this name differentiates our groups from those of other organizations in the same group management facility. The latter portion identifies the specific project, each with a Principal Investigator or other administrator, that oversees access to resources. Within our web tools, these names are often in mixed case, though you may see them uppercased in some reporting/accounting software. The first step in registration is to create a user account and bind it to other identity information. After that you will enroll in a project group. Once you've enrolled in a group, you'll have the requisite rights to log in to the submit node for the OSG Connect job scheduler, or to transfer data in and out of Stash. Submit node logins are typically via Secure Shell (SSH) using a password or a public key. We'll discuss how to connect further on.","title":"Overview of access procedure and accounting"},{"location":"overview/welcome_and_account_setup/starting-project/","text":"Join and Use a Project in OSG Connect \u00b6 Background \u00b6 The OSG Connect team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using OSG Connect. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use OSG Connect to submit jobs. The next section of this guide describes the process for joining an OSG Connect project. Joining a Project \u00b6 Project Membership via Account Creation Process (Default) \u00b6 You will be added to a project when going through the typical OSG Connect account setup process. After applying for an OSG Connect account, you will receive an email to set up a consultation meeting and confirm which 'OSG Project' your usage should be associated with. You will be prompted to provide information based on the following two scenarios: If you are the first member of your research group / team to use the OSG through OSG Connect , a new project will be created for you. You will need to provide the following information to do so: Project Name PI Name PI Email PI Organization PI Department Field of Science: (out of https://osp.unm.edu/pi-resources/nsf-research-classifications.html) Project Description If you know that other members of your research group have used OSG Connect in the past, you can likely join a pre-existing group. Provide the name of your institution and PI to the OSG Connect team (if you haven't already) and we can confirm. Based on this information, OSG Connect support staff will either create a project and add you to it, or add you to an existing project when your account is approved. Join a Project \u00b6 If you need to join an existing project (you can be a member of more than one), please email the OSG team (support@opensciencegrid.org) with your name and the project you wish to join, with PI in CC to confirm. \"Set\" your OSG Connect project \u00b6 Job submission on OSG Connect requires a project be assigned to your account on the login node. This can be set after you have been added to a project as described above. Option 1 (preferred) : To set your default project, sign in to your login node and type $ connect project You should see a list of projects that you have joined. Most often there will only be one option! Make sure the right project is highlighted and press \"enter\" to save that choice. Option 2 : If need to run jobs under a different project you are a member of (not your default), you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\" View Metrics For Your Project \u00b6 The project's resource usage appears in the OSG accounting system, GRACC . You can see the main OSG Connect dashboard here: Link to OSG Connect Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, specific users, or your institution.","title":"Join and Use a Project in OSG Connect "},{"location":"overview/welcome_and_account_setup/starting-project/#join-and-use-a-project-in-osg-connect","text":"","title":"Join and Use a Project in OSG Connect"},{"location":"overview/welcome_and_account_setup/starting-project/#background","text":"The OSG Connect team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using OSG Connect. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use OSG Connect to submit jobs. The next section of this guide describes the process for joining an OSG Connect project.","title":"Background"},{"location":"overview/welcome_and_account_setup/starting-project/#joining-a-project","text":"","title":"Joining a Project"},{"location":"overview/welcome_and_account_setup/starting-project/#project-membership-via-account-creation-process-default","text":"You will be added to a project when going through the typical OSG Connect account setup process. After applying for an OSG Connect account, you will receive an email to set up a consultation meeting and confirm which 'OSG Project' your usage should be associated with. You will be prompted to provide information based on the following two scenarios: If you are the first member of your research group / team to use the OSG through OSG Connect , a new project will be created for you. You will need to provide the following information to do so: Project Name PI Name PI Email PI Organization PI Department Field of Science: (out of https://osp.unm.edu/pi-resources/nsf-research-classifications.html) Project Description If you know that other members of your research group have used OSG Connect in the past, you can likely join a pre-existing group. Provide the name of your institution and PI to the OSG Connect team (if you haven't already) and we can confirm. Based on this information, OSG Connect support staff will either create a project and add you to it, or add you to an existing project when your account is approved.","title":"Project Membership via Account Creation Process (Default)"},{"location":"overview/welcome_and_account_setup/starting-project/#join-a-project","text":"If you need to join an existing project (you can be a member of more than one), please email the OSG team (support@opensciencegrid.org) with your name and the project you wish to join, with PI in CC to confirm.","title":"Join a Project"},{"location":"overview/welcome_and_account_setup/starting-project/#set-your-osg-connect-project","text":"Job submission on OSG Connect requires a project be assigned to your account on the login node. This can be set after you have been added to a project as described above. Option 1 (preferred) : To set your default project, sign in to your login node and type $ connect project You should see a list of projects that you have joined. Most often there will only be one option! Make sure the right project is highlighted and press \"enter\" to save that choice. Option 2 : If need to run jobs under a different project you are a member of (not your default), you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\"","title":"\"Set\" your OSG Connect project"},{"location":"overview/welcome_and_account_setup/starting-project/#view-metrics-for-your-project","text":"The project's resource usage appears in the OSG accounting system, GRACC . You can see the main OSG Connect dashboard here: Link to OSG Connect Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, specific users, or your institution.","title":"View Metrics For Your Project"},{"location":"software_examples_for_osg/freesurfer/Introduction/","text":"Introduction to FreeSurfer on the OSPool \u00b6 Overview \u00b6 FreeSurfer is a software package to analyze MRI scans of human brains. The OSG used to have a service called Fsurf, which is now discontinued. Instead we have community supported FreeSurfer container image and workflow. Please see: https://github.com/pegasus-isi/freesurfer-osg-workflow - scroll down to see the documentaion on this page. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer Prerequisites \u00b6 To use the FreeSurfer on OSG, you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) A regular OSG Connect account. Privacy and Confidentiality of Subjects \u00b6 In order to protect the privacy of your participants\u2019 scans, we request that you submit only defaced and fully deidentified scans for processing. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums . Acknowledging the OSG Consortium \u00b6 We gratefully request your acknowledgement of the OSG in publications benefiting from this service as described here .","title":"Introduction to FreeSurfer on the OSPool "},{"location":"software_examples_for_osg/freesurfer/Introduction/#introduction-to-freesurfer-on-the-ospool","text":"","title":"Introduction to FreeSurfer on the OSPool"},{"location":"software_examples_for_osg/freesurfer/Introduction/#overview","text":"FreeSurfer is a software package to analyze MRI scans of human brains. The OSG used to have a service called Fsurf, which is now discontinued. Instead we have community supported FreeSurfer container image and workflow. Please see: https://github.com/pegasus-isi/freesurfer-osg-workflow - scroll down to see the documentaion on this page. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer","title":"Overview"},{"location":"software_examples_for_osg/freesurfer/Introduction/#prerequisites","text":"To use the FreeSurfer on OSG, you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) A regular OSG Connect account.","title":"Prerequisites"},{"location":"software_examples_for_osg/freesurfer/Introduction/#privacy-and-confidentiality-of-subjects","text":"In order to protect the privacy of your participants\u2019 scans, we request that you submit only defaced and fully deidentified scans for processing.","title":"Privacy and Confidentiality of Subjects"},{"location":"software_examples_for_osg/freesurfer/Introduction/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples_for_osg/freesurfer/Introduction/#acknowledging-the-osg-consortium","text":"We gratefully request your acknowledgement of the OSG in publications benefiting from this service as described here .","title":"Acknowledging the OSG Consortium"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/","text":"Using conda to Run Python on the OSPool \u00b6 The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. Overview \u00b6 When should you use Miniconda as an installation method in OSG? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on OSG, create your installation environment on the submit server and send a zipped version to your jobs. Pre-Install Miniconda and Transfer to Jobs This guide also discusses how to \u201cpin\u201d your conda environment to create a more consistent and reproducible environment with specified versions of packages. Install Miniconda and Package for Jobs \u00b6 In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs. 1. Create a Miniconda Installation \u00b6 On the submit server, download the latest Linux miniconda installer and run it. [alice@login05]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@login05]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future. 2. Create a conda \"Environment\" With Your Packages \u00b6 (If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@login05]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@login05]$ conda create -n env-name (base)[alice@login05]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@login05]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@login05]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@login05]$ conda create -n py-data-sci (base)[alice@login05]$ conda activate py-data-sci (py-data-sci)[alice@login05]$ conda install pandas matplotlib (py-data-sci)[alice@login05]$ conda deactivate (base)[alice@login05]$ More About Miniconda \u00b6 See the official conda documentation for more information on creating and managing environments with conda . 3. Create Software Package \u00b6 Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@login05]$ Then, run this command to install the conda pack tool: (base)[alice@login05]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@login05]$ conda pack -n env-name (base)[alice@login05]$ chmod 644 env-name.tar.gz (base)[alice@login05]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz . 4. Check Size of Conda Environment Tar Archive \u00b6 The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use OSG Connect's /public folder, and a stash:/// link, as described in this guide . Please contact a research computing facilitator at support@opensciencegrid.org if you have questions about the best option for your jobs. More information is available at File Availability with Squid Web Proxy and Managing Large Data in HTC Jobs . 5. Create a Job Executable \u00b6 The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py 6. Submit Jobs \u00b6 In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above. Specificying Exact Dependency Versions \u00b6 An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@login05]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@login05]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@login05]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Using conda to Run Python on the OSPool "},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#using-conda-to-run-python-on-the-ospool","text":"The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools.","title":"Using conda to Run Python on the OSPool"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#overview","text":"When should you use Miniconda as an installation method in OSG? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on OSG, create your installation environment on the submit server and send a zipped version to your jobs. Pre-Install Miniconda and Transfer to Jobs This guide also discusses how to \u201cpin\u201d your conda environment to create a more consistent and reproducible environment with specified versions of packages.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#install-miniconda-and-package-for-jobs","text":"In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs.","title":"Install Miniconda and Package for Jobs"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#1-create-a-miniconda-installation","text":"On the submit server, download the latest Linux miniconda installer and run it. [alice@login05]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@login05]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future.","title":"1. Create a Miniconda Installation"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#2-create-a-conda-environment-with-your-packages","text":"(If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@login05]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@login05]$ conda create -n env-name (base)[alice@login05]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@login05]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@login05]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@login05]$ conda create -n py-data-sci (base)[alice@login05]$ conda activate py-data-sci (py-data-sci)[alice@login05]$ conda install pandas matplotlib (py-data-sci)[alice@login05]$ conda deactivate (base)[alice@login05]$","title":"2. Create a conda \"Environment\" With Your Packages"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#more-about-miniconda","text":"See the official conda documentation for more information on creating and managing environments with conda .","title":"More About Miniconda"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#3-create-software-package","text":"Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@login05]$ Then, run this command to install the conda pack tool: (base)[alice@login05]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@login05]$ conda pack -n env-name (base)[alice@login05]$ chmod 644 env-name.tar.gz (base)[alice@login05]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz .","title":"3. Create Software Package"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#4-check-size-of-conda-environment-tar-archive","text":"The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use OSG Connect's /public folder, and a stash:/// link, as described in this guide . Please contact a research computing facilitator at support@opensciencegrid.org if you have questions about the best option for your jobs. More information is available at File Availability with Squid Web Proxy and Managing Large Data in HTC Jobs .","title":"4. Check Size of Conda Environment Tar Archive"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#5-create-a-job-executable","text":"The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py","title":"5. Create a Job Executable"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#6-submit-jobs","text":"In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above.","title":"6. Submit Jobs"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#specificying-exact-dependency-versions","text":"An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@login05]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@login05]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@login05]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Specificying Exact Dependency Versions"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/","text":"Using Java in Jobs \u00b6 Overview \u00b6 If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool. Steps to Use Java in Jobs \u00b6 Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSG Connect access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH = $PWD /jdk-17.0.1/bin: $PATH export JAVA_HOME = $PWD /jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Using Java in Jobs "},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#using-java-in-jobs","text":"","title":"Using Java in Jobs"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#overview","text":"If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#steps-to-use-java-in-jobs","text":"Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSG Connect access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH = $PWD /jdk-17.0.1/bin: $PATH export JAVA_HOME = $PWD /jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Steps to Use Java in Jobs"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/","text":"Using Julia on the OSPool \u00b6 Overview \u00b6 This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The OSG Connect team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity Guide for more details. Quickstart Instructions \u00b6 Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the login node, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the login node, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#-#-#/bin: $PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs Install Julia Packages \u00b6 If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl . Download Julia and set up a \"project\" \u00b6 If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\". Install Packages \u00b6 We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@opensciencegrid.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/ Submit Julia Jobs \u00b6 To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages. Create Executable Bash Script \u00b6 Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ). Example Bash Script For Base Julia Only \u00b6 If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # run Julia script julia script.jl Example Bash Script For Julia With Installed Packages \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH = $_CONDOR_SCRATCH_DIR /my-project # run Julia script julia --project = my-project script.jl Create HTCondor Submit File \u00b6 After creating a bash script to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <100MB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Using Julia on the OSPool "},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#using-julia-on-the-ospool","text":"","title":"Using Julia on the OSPool"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#overview","text":"This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The OSG Connect team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity Guide for more details.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#quickstart-instructions","text":"Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the login node, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the login node, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#-#-#/bin: $PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs","title":"Quickstart Instructions"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#install-julia-packages","text":"If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl .","title":"Install Julia Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#download-julia-and-set-up-a-project","text":"If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\".","title":"Download Julia and set up a \"project\""},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#install-packages","text":"We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@opensciencegrid.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/","title":"Install Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#submit-julia-jobs","text":"To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages.","title":"Submit Julia Jobs"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#create-executable-bash-script","text":"Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ).","title":"Create Executable Bash Script"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#example-bash-script-for-base-julia-only","text":"If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # run Julia script julia script.jl","title":"Example Bash Script For Base Julia Only"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#example-bash-script-for-julia-with-installed-packages","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH = $_CONDOR_SCRATCH_DIR /my-project # run Julia script julia --project = my-project script.jl","title":"Example Bash Script For Julia With Installed Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#create-htcondor-submit-file","text":"After creating a bash script to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <100MB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Create HTCondor Submit File"},{"location":"software_examples_for_osg/python/manage-python-packages/","text":"Run Python Scripts on the OSPool \u00b6 Run Python Scripts on the OSPool \u00b6 Overview \u00b6 This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job. Running Base Python on the Open Science Pool \u00b6 Several installations of base Python are available via the Open Science Pool's Software Module System . To see what Python versions are available on the Open Science Pool run module avail while connected to our login node. Create a bash script to run Python \u00b6 To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will include commands to first load the appropriate Python module and then run our Python script called myscript.py . For example, run_py.sh : 1 2 3 4 5 6 7 #!/bin/bash # Load Python module load python/3.7.0 # Run the Python script python3 myscript.py If you need to use Python 2, load the appropriate module and replace the python3 above with python2 . Create an HTCondor submit file \u00b6 In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include requirements that request OSG nodes with access to base Python modules All together, the submit file will look something like this: universe = vanilla executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error # Require nodes that can access the correct OSG modules Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file. Running Python Jobs That Use Additional Packages \u00b6 It's likely that you'll need additional Python packages (aka libraries) that are not present in the base Python installations made available via modules. This portion of the guide describes how to create a Python \"virtual environment\" that contains your packages and which can be included as part of your jobs. Install Python packages \u00b6 While connected to your login node, load the Python module that you want to use to run jobs: $ module load python/3.7.0 Next, create a virtual environment. The first command creates a base environment: $ python3 -m venv my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . This creates a directory my_env in the current working directory with sub-directories bin/ , include/ , and lib/ . Then activate the environment and install packages to it. $ source my_env/bin/activate Notice how our command line prompt changes to: (my_env)$ The activation process redefines some of the shell variables such as PYTHON_PATH, LIBRARY_PATH etc. After activation, packages can be installed using pip which is a tool to install Python packages. (my_env)$ pip install numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. Once you are done, you can leave the virtual environment: (my_env)$ deactivate The above command resets the shell environmental variables and returns you to the normal shell prompt (with the prefix my_env removed). All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar czf my_env.tar.gz my_env Create executable script to use installed packages \u00b6 In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # Load Python # (should be the same version used to create the virtual environment) module load python/3.7.0 # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz python3 -m venv my_env source my_env/bin/activate # Run the Python script python3 myscript.py # Deactivate environment deactivate Modify the HTCondor submit file to transfer Python packages \u00b6 The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error # Require nodes that can access the correct OSG modules Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Other Considerations \u00b6 This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Run Python Scripts on the OSPool "},{"location":"software_examples_for_osg/python/manage-python-packages/#run-python-scripts-on-the-ospool","text":"","title":"Run Python Scripts on the OSPool"},{"location":"software_examples_for_osg/python/manage-python-packages/#run-python-scripts-on-the-ospool_1","text":"","title":"Run Python Scripts on the OSPool"},{"location":"software_examples_for_osg/python/manage-python-packages/#overview","text":"This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job.","title":"Overview"},{"location":"software_examples_for_osg/python/manage-python-packages/#running-base-python-on-the-open-science-pool","text":"Several installations of base Python are available via the Open Science Pool's Software Module System . To see what Python versions are available on the Open Science Pool run module avail while connected to our login node.","title":"Running Base Python on the Open Science Pool"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-a-bash-script-to-run-python","text":"To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will include commands to first load the appropriate Python module and then run our Python script called myscript.py . For example, run_py.sh : 1 2 3 4 5 6 7 #!/bin/bash # Load Python module load python/3.7.0 # Run the Python script python3 myscript.py If you need to use Python 2, load the appropriate module and replace the python3 above with python2 .","title":"Create a bash script to run Python"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-an-htcondor-submit-file","text":"In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include requirements that request OSG nodes with access to base Python modules All together, the submit file will look something like this: universe = vanilla executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error # Require nodes that can access the correct OSG modules Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file.","title":"Create an HTCondor submit file"},{"location":"software_examples_for_osg/python/manage-python-packages/#running-python-jobs-that-use-additional-packages","text":"It's likely that you'll need additional Python packages (aka libraries) that are not present in the base Python installations made available via modules. This portion of the guide describes how to create a Python \"virtual environment\" that contains your packages and which can be included as part of your jobs.","title":"Running Python Jobs That Use Additional Packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#install-python-packages","text":"While connected to your login node, load the Python module that you want to use to run jobs: $ module load python/3.7.0 Next, create a virtual environment. The first command creates a base environment: $ python3 -m venv my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . This creates a directory my_env in the current working directory with sub-directories bin/ , include/ , and lib/ . Then activate the environment and install packages to it. $ source my_env/bin/activate Notice how our command line prompt changes to: (my_env)$ The activation process redefines some of the shell variables such as PYTHON_PATH, LIBRARY_PATH etc. After activation, packages can be installed using pip which is a tool to install Python packages. (my_env)$ pip install numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. Once you are done, you can leave the virtual environment: (my_env)$ deactivate The above command resets the shell environmental variables and returns you to the normal shell prompt (with the prefix my_env removed). All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar czf my_env.tar.gz my_env","title":"Install Python packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-executable-script-to-use-installed-packages","text":"In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # Load Python # (should be the same version used to create the virtual environment) module load python/3.7.0 # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz python3 -m venv my_env source my_env/bin/activate # Run the Python script python3 myscript.py # Deactivate environment deactivate","title":"Create executable script to use installed packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#modify-the-htcondor-submit-file-to-transfer-python-packages","text":"The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error # Require nodes that can access the correct OSG modules Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1","title":"Modify the HTCondor submit file to transfer Python packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#other-considerations","text":"This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests.","title":"Other Considerations"},{"location":"software_examples_for_osg/python/manage-python-packages/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"support_and_training_resources/education_and_training/osg-user-school/","text":"Annual, Week-Long OSG User School \u00b6 Overview \u00b6 During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School. Open Materials and Recordings \u00b6 The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020 Past OSG User Schools \u00b6 OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Annual, Week-Long OSG User School "},{"location":"support_and_training_resources/education_and_training/osg-user-school/#annual-week-long-osg-user-school","text":"","title":"Annual, Week-Long OSG User School"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#overview","text":"During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School.","title":"Overview"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#open-materials-and-recordings","text":"The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020","title":"Open Materials and Recordings"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#past-osg-user-schools","text":"OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Past OSG User Schools"},{"location":"support_and_training_resources/education_and_training/osgusertraining/","text":"OSG User Training (regular/monthly) \u00b6 Sign Up for Upcoming Trainings! \u00b6 All User Training sessions are offered from 2:30-4pm ET (and usually on Tuesdays). New User Training is offered monthly, generally on the first Tuesday of the month, and training on various additional topics happens on the third Tuesday of the month. It's best to already have a user account on OSG Connect (or another access point that submits to the Open Science Pool) to follow along with hands-on examples, but anyone can listen in by registering. Tuesday, May 3: New User Training, Register here Tuesday, May 17: Using Containerized Software on the Open Science Pool, Register here Tuesday, June 7: New User Training, Register here Tuesday, June 21: Software Portability on the Open Science Pool, Register here Training Materials by Topic \u00b6 All of our training materials are public, and with recent video recordings available: New User Training (monthly) \u00b6 The most recent version of our New User Training materials are here: Slides , Video Wordcount Frequency Tutorial Organizing and Submitting HTC Workloads \u00b6 The most recent version of these training materials are here: Slides Wordcount Frequency Tutorial Using Containerized Software on the Open Science Pool \u00b6 In development (see above, and register!). Additional Topics \u00b6 As we introduce new training topics, we will add materials to this page.","title":"OSG User Training (regular/monthly) "},{"location":"support_and_training_resources/education_and_training/osgusertraining/#osg-user-training-regularmonthly","text":"","title":"OSG User Training (regular/monthly)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#sign-up-for-upcoming-trainings","text":"All User Training sessions are offered from 2:30-4pm ET (and usually on Tuesdays). New User Training is offered monthly, generally on the first Tuesday of the month, and training on various additional topics happens on the third Tuesday of the month. It's best to already have a user account on OSG Connect (or another access point that submits to the Open Science Pool) to follow along with hands-on examples, but anyone can listen in by registering. Tuesday, May 3: New User Training, Register here Tuesday, May 17: Using Containerized Software on the Open Science Pool, Register here Tuesday, June 7: New User Training, Register here Tuesday, June 21: Software Portability on the Open Science Pool, Register here","title":"Sign Up for Upcoming Trainings!"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#training-materials-by-topic","text":"All of our training materials are public, and with recent video recordings available:","title":"Training Materials by Topic"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#new-user-training-monthly","text":"The most recent version of our New User Training materials are here: Slides , Video Wordcount Frequency Tutorial","title":"New User Training (monthly)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#organizing-and-submitting-htc-workloads","text":"The most recent version of these training materials are here: Slides Wordcount Frequency Tutorial","title":"Organizing and Submitting HTC Workloads"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#using-containerized-software-on-the-open-science-pool","text":"In development (see above, and register!).","title":"Using Containerized Software on the Open Science Pool"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#additional-topics","text":"As we introduce new training topics, we will add materials to this page.","title":"Additional Topics"},{"location":"support_and_training_resources/education_and_training/previous-training-events/","text":"Other Past Training Events \u00b6 Overview \u00b6 We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@opensciencegrid.org, in case we can fix/remove them.) Workshops/Tutorials \u00b6 Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18) Tutorials at Recent OSG All-Hands Meetings \u00b6 The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Other Past Training Events "},{"location":"support_and_training_resources/education_and_training/previous-training-events/#other-past-training-events","text":"","title":"Other Past Training Events"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#overview","text":"We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@opensciencegrid.org, in case we can fix/remove them.)","title":"Overview"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#workshopstutorials","text":"Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18)","title":"Workshops/Tutorials"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#tutorials-at-recent-osg-all-hands-meetings","text":"The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Tutorials at Recent OSG All-Hands Meetings"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/","text":"Email, Office Hours, and 1-1 Meetings \u00b6 There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime! Research Computing Facilitators \u00b6 To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on OSG compute resources, but can also point you to other services related to research computing and data needs. For example, RCFs can: Assist with planning your computational approach for a research problem Teach you to submit jobs via the OSG Connect service Help you with troubleshooting on OSG systems Connect you with other researchers using similar software or methods Point to learning materials for programming and software development Help you identify applicable non-OSG data storage options Find someone who knows the answer to your question, even if the RCF doesn\u2019t \u2026 and other helpful activities to facilitate your use of cyberinfrastructure We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , user training , or web searches. RCFs are here to help! Get an Account \u00b6 If you don\u2019t have an account yet, please Sign Up , and we\u2019ll follow up quickly to set up a meeting time and create accounts. If you don\u2019t have an account but just have general questions, feel free to send an email to support@opensciencegrid.org (see below). Help via Email \u00b6 We provide ongoing support via email to support@opensciencegrid.org, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours. Virtual Office Hours \u00b6 Drop-in for live help, starting January 11, 2022! Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into an OSG Connect login node, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room. Cancellations will be announced via email. As always, if the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting. Make an Appointment \u00b6 We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@opensciencegrid.org, and we will set up a time to meet! Training Opportunities \u00b6 The RCF team runs regular new user training on the first Tuesday of the month. See upcoming training dates, registration information, and materials on the OSG Training page .","title":"Email, Office Hours, and 1-1 Meetings "},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#email-office-hours-and-1-1-meetings","text":"There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime!","title":"Email, Office Hours, and 1-1 Meetings"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#research-computing-facilitators","text":"To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on OSG compute resources, but can also point you to other services related to research computing and data needs. For example, RCFs can: Assist with planning your computational approach for a research problem Teach you to submit jobs via the OSG Connect service Help you with troubleshooting on OSG systems Connect you with other researchers using similar software or methods Point to learning materials for programming and software development Help you identify applicable non-OSG data storage options Find someone who knows the answer to your question, even if the RCF doesn\u2019t \u2026 and other helpful activities to facilitate your use of cyberinfrastructure We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , user training , or web searches. RCFs are here to help!","title":"Research Computing Facilitators"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#get-an-account","text":"If you don\u2019t have an account yet, please Sign Up , and we\u2019ll follow up quickly to set up a meeting time and create accounts. If you don\u2019t have an account but just have general questions, feel free to send an email to support@opensciencegrid.org (see below).","title":"Get an Account"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#help-via-email","text":"We provide ongoing support via email to support@opensciencegrid.org, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours.","title":"Help via Email"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#virtual-office-hours","text":"Drop-in for live help, starting January 11, 2022! Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into an OSG Connect login node, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room. Cancellations will be announced via email. As always, if the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting.","title":"Virtual Office Hours"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#make-an-appointment","text":"We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@opensciencegrid.org, and we will set up a time to meet!","title":"Make an Appointment"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#training-opportunities","text":"The RCF team runs regular new user training on the first Tuesday of the month. See upcoming training dates, registration information, and materials on the OSG Training page .","title":"Training Opportunities"}]}